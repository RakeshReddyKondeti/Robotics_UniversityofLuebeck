{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "MDL_Exercise1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13ef3526acc84f44b1f4163ca7d636eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16bc5012405f4c6b819e8d638cd613cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72445fcb67624397a14cb92cdbcf972f",
              "IPY_MODEL_7e2240ac90c94d66ae4f8b9be4a241f0"
            ]
          }
        },
        "16bc5012405f4c6b819e8d638cd613cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72445fcb67624397a14cb92cdbcf972f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2420780381bb42fda46630ab22d85fe3",
            "_dom_classes": [],
            "description": "progress:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_039d3f5fe495465ca40befd0084f5c96"
          }
        },
        "7e2240ac90c94d66ae4f8b9be4a241f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5761a3d033b248739cf523d7b8396438",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/25 [00:12&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57796abedf5b435cb241d6db9c6bc1a7"
          }
        },
        "2420780381bb42fda46630ab22d85fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "039d3f5fe495465ca40befd0084f5c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5761a3d033b248739cf523d7b8396438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57796abedf5b435cb241d6db9c6bc1a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkCyEwQy4UHB"
      },
      "source": [
        "# MDL Exercise 1\n",
        "\n",
        "---\n",
        "\n",
        "In this tutorial we will set up the foundations to load the data, create a fully-convolutional network (FCN) for image segmentation, and train it to solve multi-class segmentation problems for medical applications. On top of that, we will implement DeepLab with a MobileNetV2 backbone which will be compared to the results of the FCN."
      ],
      "id": "MkCyEwQy4UHB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX__SDCBS0bU"
      },
      "source": [
        "**Data**\n",
        "\n",
        "At first we download and unzip the data:"
      ],
      "id": "eX__SDCBS0bU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs1SnVCBv0Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ae0963-9784-4cfd-9295-386b0c9dc738"
      },
      "source": [
        "!wget https://cloud.imi.uni-luebeck.de/s/zFyEiJKNtaKKzS8/download -O AbdomenPreAffine.zip\n",
        "!unzip -o AbdomenPreAffine.zip > /dev/null  # disable the output"
      ],
      "id": "Fs1SnVCBv0Nq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 10:52:40--  https://cloud.imi.uni-luebeck.de/s/zFyEiJKNtaKKzS8/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1134479629 (1.1G) [application/zip]\n",
            "Saving to: â€˜AbdomenPreAffine.zipâ€™\n",
            "\n",
            "AbdomenPreAffine.zi 100%[===================>]   1.06G  10.8MB/s    in 1m 42s  \n",
            "\n",
            "2021-05-03 10:54:24 (10.6 MB/s) - â€˜AbdomenPreAffine.zipâ€™ saved [1134479629/1134479629]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXdIbHDpDu_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564c046c-8726-4d5d-f6e2-2a03d8e540d8"
      },
      "source": [
        "!wget https://cloud.imi.uni-luebeck.de/s/Fd63J7xMLmkMEzb/download -O mdl_exercise1_utils.py\n",
        "\n",
        "from mdl_exercise1_utils import init_weights, Plotter, ZeroPad, Crop, Scale, ToCuda"
      ],
      "id": "BXdIbHDpDu_J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 10:54:32--  https://cloud.imi.uni-luebeck.de/s/Fd63J7xMLmkMEzb/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4256 (4.2K) [text/x-python]\n",
            "Saving to: â€˜mdl_exercise1_utils.pyâ€™\n",
            "\n",
            "mdl_exercise1_utils 100%[===================>]   4.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-03 10:54:34 (589 MB/s) - â€˜mdl_exercise1_utils.pyâ€™ saved [4256/4256]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkOPbzDvbtwO"
      },
      "source": [
        "Let's get started with the code and run all the imports:"
      ],
      "id": "IkOPbzDvbtwO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fresh-details"
      },
      "source": [
        "#!pip uninstall torch -y\n",
        "#!pip install torch -y\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import nibabel as nib\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from typing import Callable, Any, Optional, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "#!nvcc --version"
      ],
      "id": "fresh-details",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tn1AiXHIo_6"
      },
      "source": [
        "The **training data** is loaded from the filesystem as:"
      ],
      "id": "_Tn1AiXHIo_6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOUzC-WCMIlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6147df-65a1-4a19-e2df-546276231c17"
      },
      "source": [
        "imgs2 = torch.randn(20,1,128,128,128).cuda()#/500\n",
        "segs2 = torch.randint(1,(20,128,128,128)).long().cuda()\n",
        "list_train = torch.Tensor([2,3,4,5,7,8,10,21,22,24,25,27,28,30,31,33,34,36,37,39,40])\n",
        "list_test = torch.Tensor([1,4,7,10,23,26,29,32,35,38]).long()\n",
        "for i in range(20):\n",
        "    img = nib.load('/content/AbdomenPreAffine/Training/img/img00'+str(int(list_train[i])).zfill(2)+'.nii.gz').get_fdata()\n",
        "    imgs2[i:i+1] = F.interpolate(torch.from_numpy(img).cuda().unsqueeze(0).unsqueeze(1).float(),size=(128,128,128),mode='trilinear').cuda()/500\n",
        "    seg = nib.load('/content/AbdomenPreAffine/Training/label/label00'+str(int(list_train[i])).zfill(2)+'.nii.gz').get_fdata()\n",
        "    segs2[i] = F.interpolate(torch.from_numpy(seg).cuda().unsqueeze(0).unsqueeze(1).float(),size=(128,128,128),mode='nearest').squeeze().cuda().long()\n",
        "    print('Loaded', i+1, '/',20)"
      ],
      "id": "YOUzC-WCMIlj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 1 / 20\n",
            "Loaded 2 / 20\n",
            "Loaded 3 / 20\n",
            "Loaded 4 / 20\n",
            "Loaded 5 / 20\n",
            "Loaded 6 / 20\n",
            "Loaded 7 / 20\n",
            "Loaded 8 / 20\n",
            "Loaded 9 / 20\n",
            "Loaded 10 / 20\n",
            "Loaded 11 / 20\n",
            "Loaded 12 / 20\n",
            "Loaded 13 / 20\n",
            "Loaded 14 / 20\n",
            "Loaded 15 / 20\n",
            "Loaded 16 / 20\n",
            "Loaded 17 / 20\n",
            "Loaded 18 / 20\n",
            "Loaded 19 / 20\n",
            "Loaded 20 / 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BpfYYqgIyYD"
      },
      "source": [
        "The **validation data** is loaded from the filesystem as:"
      ],
      "id": "7BpfYYqgIyYD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRSfkzznI9Mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf0db21-9ed3-4e6f-9dbb-f1e370078a20"
      },
      "source": [
        "imgs_val = torch.randn(10,1,128,128,128).cuda()#/500\n",
        "segs_val = torch.randint(2,(10,128,128,128)).long().cuda()\n",
        "for i in range(10):\n",
        "    img = nib.load('/content/AbdomenPreAffine/Training/img/img00'+str(int(list_test[i])).zfill(2)+'.nii.gz').get_fdata()\n",
        "    imgs_val[i:i+1] = F.interpolate(torch.from_numpy(img).unsqueeze(0).unsqueeze(1).float(),size=(128,128,128),mode='trilinear').cuda()/500\n",
        "    seg = nib.load('/content/AbdomenPreAffine/Training/label/label00'+str(int(list_test[i])).zfill(2)+'.nii.gz').get_fdata()\n",
        "    segs_val[i] = F.interpolate(torch.from_numpy(seg).unsqueeze(0).unsqueeze(1).float(),size=(128,128,128),mode='nearest').squeeze().cuda().long()\n",
        "    print('Loaded', i+1, '/',10)"
      ],
      "id": "aRSfkzznI9Mc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 1 / 10\n",
            "Loaded 2 / 10\n",
            "Loaded 3 / 10\n",
            "Loaded 4 / 10\n",
            "Loaded 5 / 10\n",
            "Loaded 6 / 10\n",
            "Loaded 7 / 10\n",
            "Loaded 8 / 10\n",
            "Loaded 9 / 10\n",
            "Loaded 10 / 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crO5MzCTJ0D_"
      },
      "source": [
        "All data is being loaded now, thus we can start with the implementation of the following tasks."
      ],
      "id": "crO5MzCTJ0D_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6QRNRqRx9vP"
      },
      "source": [
        "---\n",
        "\n",
        "**Task 1 (3 points)**: Now you can write your own implementation for an affine augmentation using the `nn.functional` package (already imported in the first cell as `F.`):\n",
        "\n",
        "âœ” Define random offsets added to an affine identity matrix\n",
        "\n",
        "âœ” Transform the matrix to a PyTorch grid using `F.affine_grid()`\n",
        "\n",
        "âœ” Resample image and label using `F.grid_sample()`"
      ],
      "id": "H6QRNRqRx9vP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRs0tKd1xJ7H"
      },
      "source": [
        "class AugmentAffine(object):\n",
        "  def __init__(self, strength=0.05):\n",
        "    self.strength = strength\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    B, C, D, H, W = sample['image'].size()# TODO access image data\n",
        "    \n",
        "    offsets = torch.rand(3,4)# TODO random offsets\n",
        "    \n",
        "    affine_matrix = (torch.eye(3, 4).unsqueeze(0) + self.strength * offsets)  \n",
        "    affine_matrix.cuda()\n",
        "\n",
        "    \n",
        "\n",
        "    meshgrid = F.affine_grid(affine_matrix, torch.Size((B, C, D, H, W))).cuda()# TODO resampling grid\n",
        "\n",
        "    #sample['image'].resize_(1,C, D, H, W)\n",
        "\n",
        "    sample['image'] = F.grid_sample(sample['image'], meshgrid)# TODO resample image\n",
        "    sample['label'] = F.grid_sample(sample['label'].unsqueeze(1).type(torch.float), meshgrid)# TODO resample label\n",
        "    sample['label'] = sample['label'].squeeze(1).long()\n",
        "    \n",
        "    return sample\n",
        "  \n",
        "  \n",
        "augmentation_training = [AugmentAffine(0.1), ToCuda()]\n",
        "augmentation_validate = [ToCuda()]"
      ],
      "id": "JRs0tKd1xJ7H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9LFJifZqiM4"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Task 2 (7 Points)**: Define a simplified Fully-Convolutional Network (FCN) architecture:\n",
        "\n",
        "âœ” Use double 3x3 convolutional blocks, such as in `block0`\n",
        "\n",
        "âœ” 3 poolings, and 1 final upsampling\n",
        "\n",
        "âœ” Double the channel number, half the spatial resolution\n",
        "\n",
        "âœ” To keep it easy: Use padding, so size of feature map after convolution is same as before\n",
        "\n"
      ],
      "id": "p9LFJifZqiM4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtuaD78Yda2y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, unet=True):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.block0 = nn.Sequential(nn.BatchNorm3d(1),\n",
        "                                nn.Conv3d(1, 16, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.BatchNorm3d(16),\n",
        "                                nn.Conv3d(16, 16, 3, padding=1),\n",
        "                                nn.ReLU()\n",
        "                               )\n",
        "                    \n",
        "    self.mp01 = nn.MaxPool3d(2, 2)\n",
        "    \n",
        "    # TODO add two more blocks and two more 2x2 poolings\n",
        "    self.block1 = nn.Sequential(nn.BatchNorm3d(16),\n",
        "                                nn.Conv3d(16, 32, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.BatchNorm3d(32),\n",
        "                                nn.Conv3d(32, 32, 3, padding=1),\n",
        "                                nn.ReLU()\n",
        "                               )\n",
        "                    \n",
        "    self.mp02 = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    self.block2 = nn.Sequential(nn.BatchNorm3d(32),\n",
        "                                nn.Conv3d(32, 64, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.BatchNorm3d(64),\n",
        "                                nn.Conv3d(64, 64, 3, padding=1),\n",
        "                                nn.ReLU()\n",
        "                               )\n",
        "                    \n",
        "    self.mp03 = nn.MaxPool3d(2, 2)\n",
        "    \n",
        "    \n",
        "    # TODO add final classifiation block (1x1 convs instead of linear layers)\n",
        "    self.classifier = nn.Sequential(\n",
        "          nn.Conv3d(in_channels=64, out_channels=14, kernel_size=1),\n",
        "          nn.Softmax()\n",
        "         )\n",
        "    \n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    output0 = self.block0(inputs)\n",
        "    output1 = self.mp01(output0)\n",
        "    output2 = self.block1(output1)\n",
        "    output3 = self.mp02(output2)\n",
        "    output4 = self.block2(output3)\n",
        "    output5 = self.mp03(output4)\n",
        "    final_output = self.classifier(output5)\n",
        "    # TODO Add forwards\n",
        "    \n",
        "    return F.interpolate(final_output, scale_factor=8, mode=\"trilinear\")  # TODO Add both values"
      ],
      "id": "ZtuaD78Yda2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dNlP1_ZIac"
      },
      "source": [
        "Let's set up the experiment.\n",
        "\n",
        "First, we need the network model (`net`). The network weights need to be initialized with weights near 1 to avoid vanishing/exploding gradients issues that can later slow down the learning process.\n",
        "\n",
        "Training of neural networks is an optimsation process. Thus, we further need:\n",
        "\n",
        "1.  Parameters of a model to be optimized: ```net.parameters()```\n",
        "2.  Optimization criterion: ```nn.BCELoss``` (binary cross-entropy)\n",
        "3.  Optimizer adapting parameters of (1.) in order to minimize loss (2.): ```torch.optim.Adam```\n",
        "\n",
        "One hyper-parameter of such gradient-descent optimisation is the number of iterations:\n",
        "\n",
        "1.  Number of epochs: ```n_epochs``` (one epoch: processing all training sample batches once)"
      ],
      "id": "a5dNlP1_ZIac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqWOKygAtGs"
      },
      "source": [
        "After the set up, we just run the main training routine and watch the loss going down!"
      ],
      "id": "7UqWOKygAtGs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEdeRNwSqbwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "13ef3526acc84f44b1f4163ca7d636eb",
            "16bc5012405f4c6b819e8d638cd613cf",
            "72445fcb67624397a14cb92cdbcf972f",
            "7e2240ac90c94d66ae4f8b9be4a241f0",
            "2420780381bb42fda46630ab22d85fe3",
            "039d3f5fe495465ca40befd0084f5c96",
            "5761a3d033b248739cf523d7b8396438",
            "57796abedf5b435cb241d6db9c6bc1a7"
          ]
        },
        "outputId": "668c4885-8744-413f-a91c-3b80a6219c36"
      },
      "source": [
        "# Hyper-parameters\n",
        "n_epochs = 25\n",
        "\n",
        "# Visualise progress every 5th epoch\n",
        "every_epoch = 5\n",
        "bin_thresh = 0.3\n",
        "plotter = Plotter(n_epochs//every_epoch, z_slice=23, bin_thresh=bin_thresh)\n",
        "\n",
        "# Network initialisation\n",
        "net = Net(False).cuda()\n",
        "net.apply(init_weights)\n",
        "\n",
        "# Set up optimisation for training process\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "# Save loss of each epoch\n",
        "losses_training = []\n",
        "losses_validate = []\n",
        "\n",
        "# Show current progress and loss\n",
        "progress = tqdm(range(n_epochs), desc='progress')\n",
        "\n",
        "data_aug = AugmentAffine()\n",
        "\n",
        "\n",
        "######################\n",
        "# MAIN TRAINING LOOP #\n",
        "######################\n",
        "\n",
        "\n",
        "for epoch in progress:\n",
        "  \n",
        "  ########################################\n",
        "  #               TRAINING               #\n",
        "  ########################################\n",
        "\n",
        "  sum_loss = 0\n",
        "  \n",
        "  # Parameters must be trainable\n",
        "  net.train()\n",
        "  with torch.set_grad_enabled(True):  # TODO add bool value\n",
        "    \n",
        "    # loop to process all training samples (packed into batches)\n",
        "    for sample_num in range(len(imgs2)):# TODO draw training sample\n",
        "      sample = {\"image\":imgs2[sample_num][None, ...], \"label\":segs2[sample_num][None, ...]}\n",
        "      sample = data_aug(sample)\n",
        "\n",
        "      result = net(sample[\"image\"].cuda()).cuda()# TODO forward run with sample\n",
        "      loss = criterion(result.cuda(), sample[\"label\"].cuda()).cuda()# TODO compute BCE loss\n",
        "      sample[\"label\"] = sample[\"label\"].unsqueeze(1)\n",
        "\n",
        "      # TODO backward step to compute gradients for optimising the model weights\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      sum_loss += loss.item()\n",
        "  \n",
        "  losses_training.append(sum_loss / len(imgs2))# TODO add number of training samples)\n",
        "\n",
        "  if epoch % every_epoch == 0:\n",
        "    plotter.add_training_sample(sample, result, epoch)\n",
        "  \n",
        "  \n",
        "  ########################################\n",
        "  #              VALIDATION              #\n",
        "  ########################################\n",
        "\n",
        "  sum_loss = 0\n",
        "  \n",
        "  # Parameters must not be trainable\n",
        "  net.eval()\n",
        "  with torch.set_grad_enabled(False):  # TODO add bool value\n",
        "    \n",
        "    # loop to process all validation samples (packed into batches)\n",
        "    for sample_num in range(len(imgs_val)):# TODO draw validation sample\n",
        "      \n",
        "      # TODO copy and paste the lines required from the training step\n",
        "      sample = {\"image\":imgs_val[sample_num][None, ...], \"label\":segs_val[sample_num][None, ...]}\n",
        "      sample = data_aug(sample)\n",
        "\n",
        "      result = net(sample[\"image\"])# TODO forward run with sample\n",
        "\n",
        "      loss = criterion(result, sample[\"label\"])# TODO compute BCE loss\n",
        "      sample[\"label\"] = sample[\"label\"].unsqueeze(1)\n",
        "      \n",
        "      sum_loss += loss.item()\n",
        "  \n",
        "  losses_validate.append(sum_loss / len(imgs_val))# TODO add number of validation samples))\n",
        "  \n",
        "  if epoch % every_epoch == 0:\n",
        "    plotter.add_validation_sample(sample, result, epoch)\n",
        "  \n",
        "  progress.set_postfix(loss=losses_training[-1], val_loss=losses_validate[-1])"
      ],
      "id": "eEdeRNwSqbwq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13ef3526acc84f44b1f4163ca7d636eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='progress', max=25.0, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3891: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3829: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8bf1805a4d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mlosses_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# TODO add number of training samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x1800 with 36 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU_pXYTRs-m8"
      },
      "source": [
        "Let's first check if training has converged. We plot the loss over all training epochs.\n",
        "\n",
        "Thus, we compare the training and validation loss curves:"
      ],
      "id": "jU_pXYTRs-m8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGhomWJLpcp0"
      },
      "source": [
        "plt.plot(losses_training, 'r-', linewidth=2)  # training loss in red\n",
        "plt.plot(losses_validate, 'b-', linewidth=2)  # validation loss in blue"
      ],
      "id": "IGhomWJLpcp0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOtERFa0tikt"
      },
      "source": [
        "Reaching a plateau means, we can hopefully roughly see the segmentation matching the ground truth mask.\n",
        "\n",
        "The following plot rows have been generated every fifth epoch:"
      ],
      "id": "bOtERFa0tikt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dCpMZ4G7kH0"
      },
      "source": [
        "def dice_val(f_label, m_label):\n",
        "  numerator = 2.0*torch.sum(f_label.view(-1) * m_label.view(-1))\n",
        "  denominator = torch.sum(f_label.view(-1)) + torch.sum(m_label.view(-1))\n",
        "  return numerator / denominator\n",
        "\n",
        "sum_dice = 0\n",
        "net.train(False)\n",
        "with torch.set_grad_enabled(False):\n",
        "  for sample_num in range(len(imgs_val)):# TODO draw validation sample\n",
        "    sample = {\"image\":imgs_val[sample_num][None, ...], \"label\":segs_val[sample_num][None, ...]}\n",
        "    sample = data_aug(sample)\n",
        "    result = net(sample['image'])  # forward run\n",
        "    sample[\"label\"] = sample[\"label\"].unsqueeze(1)\n",
        "    sum_dice += float(dice_val((result > bin_thresh).float(), sample['label'].float()))\n",
        "print('Dice validation:', sum_dice/len(imgs_val) )"
      ],
      "id": "3dCpMZ4G7kH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qr4Dc6WNGJQ"
      },
      "source": [
        "**Task 3 (4 Points)**: Define the MobileNetV2 architecture\n",
        "\n",
        "âœ” Implement a linear bottleneck module\n",
        "\n",
        "âœ” Implement a inverted residual block\n",
        "\n",
        "Please have a look at this paper https://arxiv.org/abs/1801.04381 and the exercise slides for reference.\n",
        "\n"
      ],
      "id": "3Qr4Dc6WNGJQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ur_RCUPM8Kk"
      },
      "source": [
        "At first we implement the **bottleneck module** for MobileNet:"
      ],
      "id": "4ur_RCUPM8Kk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYZNeKSGAIiF"
      },
      "source": [
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm3d# TODO set required layer\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6# TODO set required layer\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv3d(in_planes, out_planes, 3, stride, 1, bias=False),# TODO add Conv3d\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "# helper function\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v"
      ],
      "id": "tYZNeKSGAIiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDfy5MXWAisf"
      },
      "source": [
        "Implement the **inverted residual block** for MobileNet:"
      ],
      "id": "qDfy5MXWAisf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "american-exploration"
      },
      "source": [
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm3d# TODO set required layer\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # TODO add the just generated bottleneck relu (Set appropriate kernel size & input dimension)\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, 1, 1, 0))\n",
        "        layers.extend([\n",
        "            # TODO add the just generated bottleneck relu (What is the input dimension here?)\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, 3, stride, hidden_dim),\n",
        "            nn.Conv3d(hidden_dim, oup, 1,1,0,bias=False),# TODO add conv3d\n",
        "            # ...\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self.is_strided = stride > 1\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            # TODO add return for residual \n",
        "            return x + self.conv() \n",
        "        else:\n",
        "            # TODO add alternative return \n",
        "            return x\n"
      ],
      "id": "american-exploration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd5GPdu3A9E2"
      },
      "source": [
        "Now we have all preliminaries for defining the **MobileNetV2** utilizing our previoulsy implemented residual and bottleneck modules: "
      ],
      "id": "wd5GPdu3A9E2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9LpJUl3AC4_"
      },
      "source": [
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1000,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual# TODO add required layer \n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm3d# TODO add required layer \n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "          inverted_residual_setting = [\n",
        "              # t, c, n, s\n",
        "              [1, 16, 1, 1],\n",
        "              [6, 24, 2, 2],\n",
        "              [6, 32, 3, 2],\n",
        "              [6, 64, 4, 2],\n",
        "              [6, 96, 3, 1],\n",
        "              [6, 160, 3, 2],\n",
        "              [6, 320, 1, 1],\n",
        "          ]\n",
        "\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # build the first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        \n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        \n",
        "        self.features: List[nn.Module] = [ConvBNReLU(3, input_channel, 2)]# add bottleneck module\n",
        "        \n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            \n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "        \n",
        "        # building last several layers\n",
        "        self.features.append(ConvBNActivation(self.last_channel, output_channel, 1, 1, 0)) # TODO add bottleneck\n",
        "        \n",
        "        self.features = nn.Sequential(*self.features)# TODO make it nn.Sequential\n",
        "\n",
        "        # TODO building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),# TODO add a dropout\n",
        "            # ... \n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    # This is a helper function bypassing forward due to restrictions in inheritance\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        x = self.features(x)# TODO set to input features\n",
        "\n",
        "        # TODO use adaptive average pooling on x (Hint: Use reshape)\n",
        "        aap = nn.AdaptiveAvgPool3d()\n",
        "        x = aap(x).reshape(x.shape[0], -1)# ...\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "# setup mobilenet\n",
        "def mobile3dnet(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV2:\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n"
      ],
      "id": "C9LpJUl3AC4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riop9RmVNpfG"
      },
      "source": [
        "**Task 4 (6 Points)**: Implement ASPP\n",
        "\n",
        "âœ” Implement the ASPPConv block defined as a 3x3 convolution followed by BatchNorm3d and ReLU\n",
        "\n",
        "âœ” Implement the ASPPPooling block consisting of a AdaptiveAvgPool3d followed by Conv3d, BatchNorm3d and ReLU blocks\n",
        "\n",
        "âœ” Setup ASPP with atrous rates of 2, 4, 8 and 16\n",
        "\n",
        "âœ” Train your DeepLabV3 network with MobileNetV2 as backbone \n",
        "\n",
        "âœ” Evaluate, visualize and compare the results to the FCNâ€™s output\n",
        "\n",
        "Please also have a look at the paper https://arxiv.org/pdf/1706.05587.pdf and exercise slides.\n"
      ],
      "id": "riop9RmVNpfG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK4gc1G3Cqml"
      },
      "source": [
        "Implement the **ASPP convolutional** as well as the **ASPP pooling** blocks as follows:"
      ],
      "id": "rK4gc1G3Cqml"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N12iMC-0B5Tv"
      },
      "source": [
        "class ASPPConv(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, dilation):\n",
        "        modules = [\n",
        "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU()\n",
        "            # TODO add conv3d, batchnorm and relu modules (Remember setting the dilation)\n",
        "            # ...\n",
        "\n",
        "        ]\n",
        "        super(ASPPConv, self).__init__(*modules)\n",
        "\n",
        "\n",
        "class ASPPPooling(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ASPPPooling, self).__init__(\n",
        "            # TODO add conv3d, batchnorm and relu modules\n",
        "            # ...\n",
        "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[-2:]# set the size of x    \n",
        "        aap = nn.AdaptiveAvgPool3d(1)       \n",
        "        x = aap(x)# TODO use adaptive average pooling\n",
        "        for mod in self:\n",
        "            x = mod(x)\n",
        "        # TODO finish this, use nearest neighbour interpolation)\n",
        "        return F.interpolate(x, size=size, mode='nearest')"
      ],
      "id": "N12iMC-0B5Tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVsXqsXmDLNk"
      },
      "source": [
        "Now we can define **ASPP** while making use of ASPPConv and ASPPPooling blocks:"
      ],
      "id": "aVsXqsXmDLNk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPzSRsmbDRhN"
      },
      "source": [
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
        "        super(ASPP, self).__init__()\n",
        "        modules = []\n",
        "        modules.append(nn.Sequential(\n",
        "            # TODO add conv3d, batchnorm and relu modules\n",
        "            # ...\n",
        "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU()))\n",
        "        \n",
        "        rates = tuple(atrous_rates)\n",
        "        for rate in rates:\n",
        "            # TODO add ASPPConv blocks with corresponding atrous rate \n",
        "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
        "        \n",
        "        # TODO add ASPPPooling\n",
        "        modules.append(ASPPPooling(in_channels, out_channels))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            # TODO add conv3d, batchnorm, relu & a final dropout\n",
        "            # ...\n",
        "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        # TODO concatenate res\n",
        "        res =  torch.cat(res, dim=1)# ...\n",
        "        return self.project(res)"
      ],
      "id": "IPzSRsmbDRhN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFwDRIU2EmLs"
      },
      "source": [
        "We define our **DeepLabV3** network utilizing **ASPP** with the MobileNetV2 serving as our backbone:"
      ],
      "id": "tFwDRIU2EmLs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "increased-garlic"
      },
      "source": [
        "# initialize backbone feature extractor (MobileNet)\n",
        "backbone = mobile3dnet(width_mult=0.75).features[:11]\n",
        "\n",
        "backbone[0][0].weight.data = backbone[0][0].weight.data[:,:1]\n",
        "backbone[0][0].in_channels = 1\n",
        "\n",
        "# TODO set stride for corresponding layer \n",
        "backbone[4].conv[1][0].stride = (1,1,1)# ...\n",
        "\n",
        "for i in range(5,7):\n",
        "    # TODO set dilatin and padding for corresponding layers\n",
        "    backbone[i].conv[1][0].dilation = (2,2,2)# ...\n",
        "    backbone[i].conv[1][0].padding = (2,2,2)# ...\n",
        "\n",
        "# TODO add stride\n",
        "backbone[7].conv[1][0].stride = (1,1,1)# ...\n",
        "\n",
        "for i in range(8,11):\n",
        "  # TODO set dilatin and padding for corresponding layers\n",
        "    backbone[i].conv[1][0].dilation = (4,4,4)# ...\n",
        "    backbone[i].conv[1][0].padding = (4,4,4)# ...\n",
        "\n",
        "# TODO initialize ASPP with correct atrious rates    \n",
        "aspp = ASPP(24, (2, 4, 8, 16), out_channels=64)\n",
        "\n",
        "x = torch.randn(2,1,96,96,96)\n",
        "\n",
        "num_classes = 14\n",
        "head = nn.Sequential(nn.Conv3d(64, 64, 3, padding=1, bias=False),nn.BatchNorm3d(64),nn.ReLU(),\\\n",
        "                     nn.Conv3d(64, num_classes, 1))"
      ],
      "id": "increased-garlic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jwc0fs8y4_x"
      },
      "source": [
        "print(imgs2[0].size())"
      ],
      "id": "9jwc0fs8y4_x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "about-champion",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3e0572c-547f-47b6-8811-ca60fd621a68"
      },
      "source": [
        "# init backbone, ASPP and the head\n",
        "backbone.cuda()\n",
        "aspp.cuda()\n",
        "head.cuda()\n",
        "\n",
        "# TODO setup an Adam optimizer\n",
        "# Hint: You can read the required parameters from backbone/aspp as with the parameters() function \n",
        "optimizer = torch.optim.Adam(aspp.parameters(), lr=0.0001)\n",
        "\n",
        "# A scaler is being used \n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "import time\n",
        "t0 = time.time()\n",
        "run_loss = torch.zeros(100) \n",
        "for i in range(0, 10):\n",
        "    idx = torch.randperm(20)[:4]\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # TODO add affine augmentation\n",
        "        affine_matrix = ((torch.eye(3, 4)).unsqueeze(0) + torch.randn(4,3,4)*0.05)\n",
        "        affine = F.affine_grid(affine_matrix, (4, 1, 128, 128, 128))\n",
        "        input = F.grid_sample(imgs2[idx],affine.cuda(),padding_mode='border')\n",
        "        target = F.grid_sample(segs2[idx].float().unsqueeze(1),affine.cuda(),mode='nearest').squeeze(1).long()\n",
        "    with torch.cuda.amp.autocast():\n",
        "        # TODO read x as output of head (subsequent to aspp and backbone)\n",
        "        b = checkpoint(backbone,input).cuda()\n",
        "        print(backbone)\n",
        "        del backbone, input\n",
        "        torch.cuda.empty_cache()\n",
        "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "        print(aspp)\n",
        "        a = checkpoint(aspp,b)\n",
        "        print(a.shape)\n",
        "        x = head(aspp(backbone(input)))# ...\n",
        "        # TODO upscale x by factor of 4 using trinilear interpolation\n",
        "        output = F.interpolate(x, scale_factor=4, mode=\"trilinear\")# ...\n",
        "        # TODO add cross entropy loss \n",
        "        loss = nn.CrossEntropyLoss().cuda()# ...\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    # Updates the scale for next iteration.\n",
        "    scaler.update()\n",
        "\n",
        "    run_loss[i] = loss.item()\n",
        "\n",
        "    if(i % 10 == 0):\n",
        "        print(i,time.time()-t0,'sec','loss',run_loss[i-8:i-1].mean())"
      ],
      "id": "about-champion",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6471 MB |    7451 MB |   75977 MB |   69506 MB |\n",
            "|       from large pool |    6464 MB |    7448 MB |   75878 MB |   69413 MB |\n",
            "|       from small pool |       6 MB |       7 MB |      99 MB |      92 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6471 MB |    7451 MB |   75977 MB |   69506 MB |\n",
            "|       from large pool |    6464 MB |    7448 MB |   75878 MB |   69413 MB |\n",
            "|       from small pool |       6 MB |       7 MB |      99 MB |      92 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    6492 MB |    7516 MB |   17434 MB |   10942 MB |\n",
            "|       from large pool |    6484 MB |    7508 MB |   17426 MB |   10942 MB |\n",
            "|       from small pool |       8 MB |       8 MB |       8 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   20986 KB |  156198 KB |   10540 MB |   10520 MB |\n",
            "|       from large pool |   19887 KB |  150996 KB |   10425 MB |   10406 MB |\n",
            "|       from small pool |    1099 KB |    5715 KB |     115 MB |     114 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     358    |     360    |    3799    |    3441    |\n",
            "|       from large pool |      25    |      33    |    1087    |    1062    |\n",
            "|       from small pool |     333    |     342    |    2712    |    2379    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     358    |     360    |    3799    |    3441    |\n",
            "|       from large pool |      25    |      33    |    1087    |    1062    |\n",
            "|       from small pool |     333    |     342    |    2712    |    2379    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      18    |      28    |      58    |      40    |\n",
            "|       from large pool |      14    |      24    |      54    |      40    |\n",
            "|       from small pool |       4    |       4    |       4    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      30    |    1823    |    1816    |\n",
            "|       from large pool |       3    |       6    |     450    |     447    |\n",
            "|       from small pool |       4    |      27    |    1373    |    1369    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3891: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3829: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): ConvBNActivation(\n",
            "    (0): Conv3d(1, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "    (1): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU6(inplace=True)\n",
            "  )\n",
            "  (1): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv3d(24, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (2): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(16, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(96, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (3): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(144, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (4): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(144, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (5): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(144, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (6): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(144, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (7): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(144, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (8): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(48, 288, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(288, 288, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(288, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (9): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(48, 288, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(288, 288, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(288, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (10): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): ConvBNActivation(\n",
            "        (0): Conv3d(48, 288, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): ConvBNActivation(\n",
            "        (0): Conv3d(288, 288, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
            "        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv3d(288, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (3): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6811 MB |    7451 MB |   77210 MB |   70398 MB |\n",
            "|       from large pool |    6808 MB |    7448 MB |   77110 MB |   70302 MB |\n",
            "|       from small pool |       3 MB |       7 MB |      99 MB |      96 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6811 MB |    7451 MB |   77210 MB |   70398 MB |\n",
            "|       from large pool |    6808 MB |    7448 MB |   77110 MB |   70302 MB |\n",
            "|       from small pool |       3 MB |       7 MB |      99 MB |      96 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    6876 MB |    7516 MB |   18458 MB |   11582 MB |\n",
            "|       from large pool |    6868 MB |    7508 MB |   18450 MB |   11582 MB |\n",
            "|       from small pool |       8 MB |       8 MB |       8 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   65931 KB |  156198 KB |   10784 MB |   10719 MB |\n",
            "|       from large pool |   61440 KB |  150996 KB |   10665 MB |   10605 MB |\n",
            "|       from small pool |    4491 KB |    5715 KB |     118 MB |     114 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     180    |     364    |    3816    |    3636    |\n",
            "|       from large pool |      15    |      33    |    1098    |    1083    |\n",
            "|       from small pool |     165    |     342    |    2718    |    2553    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     180    |     364    |    3816    |    3636    |\n",
            "|       from large pool |      15    |      33    |    1098    |    1083    |\n",
            "|       from small pool |     165    |     342    |    2718    |    2553    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      19    |      28    |      64    |      45    |\n",
            "|       from large pool |      15    |      24    |      60    |      45    |\n",
            "|       from small pool |       4    |       4    |       4    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      22    |      30    |    1849    |    1827    |\n",
            "|       from large pool |       3    |       6    |     456    |     453    |\n",
            "|       from small pool |      19    |      27    |    1393    |    1374    |\n",
            "|===========================================================================|\n",
            "\n",
            "ASPP(\n",
            "  (convs): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv3d(24, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (1): ASPPConv(\n",
            "      (0): Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (2): ASPPConv(\n",
            "      (0): Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (3): ASPPConv(\n",
            "      (0): Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(8, 8, 8), dilation=(8, 8, 8), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (4): ASPPConv(\n",
            "      (0): Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(16, 16, 16), dilation=(16, 16, 16), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (5): ASPPPooling(\n",
            "      (0): Conv3d(24, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (project): Sequential(\n",
            "    (0): Conv3d(384, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
            "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1d68744e9e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabbreviated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected keyword arguments: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b90a04b63055>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# TODO concatenate res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2150\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m     )\n\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 11.17 GiB total capacity; 10.65 GiB already allocated; 28.81 MiB free; 10.71 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MAMOQmHDpXi"
      },
      "source": [
        "Plot the loss:"
      ],
      "id": "4MAMOQmHDpXi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clinical-quarter"
      },
      "source": [
        "plt.plot(torch.log(F.avg_pool1d(run_loss.view(1,1,-1),15,stride=1).squeeze()));\n",
        "plt.show()"
      ],
      "id": "clinical-quarter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewSDtkhODzQN"
      },
      "source": [
        "Run the **validation**:"
      ],
      "id": "ewSDtkhODzQN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worth-weekly"
      },
      "source": [
        "def dice_coeff(outputs, labels, max_label):\n",
        "    dice = torch.FloatTensor(max_label-1).fill_(0)\n",
        "    for label_num in range(1, max_label):\n",
        "        iflat = (outputs==label_num).view(-1).float()\n",
        "        tflat = (labels==label_num).view(-1).float()\n",
        "        intersection = torch.mean(iflat * tflat)\n",
        "        dice[label_num-1] = (2. * intersection) / (1e-8 + torch.mean(iflat) + torch.mean(tflat))\n",
        "    return dice\n",
        "\n",
        "# validation\n",
        "head.eval()\n",
        "backbone.eval()\n",
        "aspp.eval()\n",
        "\n",
        "d_all = torch.zeros(10,13)\n",
        "predict_all = torch.zeros(10,128,128,128).short()\n",
        "for i in range(10):\n",
        "    with torch.no_grad():\n",
        "        # TODO read x from head (having passed aspp & backbone)\n",
        "        x = head(aspp(backbone(input)))# ...\n",
        "        # TODO upscale x by factor of 4 \n",
        "        output = F.interpolate(x, scale_factor=4, mode=\"trilinear\")# ...\n",
        "    \n",
        "    predict_all[i] = output.argmax(1).cpu().data\n",
        "    d = dice_coeff(output.argmax(1).cpu().data,segs_val[i:i+1].cpu().data,14)\n",
        "    \n",
        "    d_all[i,:] = d\n",
        "  \n",
        "label_txt = ['spleen','right kidney','left kidney','esophagus','liver','stomach','inferior vena cava',\\\n",
        "             'portal vein','pancreas']\n",
        "\n",
        "label_rgb = torch.Tensor([0,0,0, 208,0,0, 255,230,2, 48,194,0, 0,0,0, 0,110,255, 98,0,190, 247,0,255, 158,97,0, 255,160,70, \\\n",
        "                         0,131,32, 49,165,171]).view(12,3)\n"
      ],
      "id": "worth-weekly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOsjgW2GESla"
      },
      "source": [
        "Inspect the **loss** and **dice** for the invidual classes:"
      ],
      "id": "nOsjgW2GESla"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9Ra3H2EeQ-"
      },
      "source": [
        "# TODO evaluation"
      ],
      "id": "th9Ra3H2EeQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwWTwstMEgn_"
      },
      "source": [
        "The following function can be used to overlay the predicted segmentation onto the input images:"
      ],
      "id": "BwWTwstMEgn_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quantitative-messaging"
      },
      "source": [
        "def overlaySegment(gray1,seg1,colors,flag=False):\n",
        "    H, W = seg1.squeeze().size()\n",
        "    segs1 = F.one_hot(seg1.long(),14).float().permute(2,0,1)[:12] \n",
        "\n",
        "    seg_color = torch.mm(segs1.view(12,-1).t(),colors[:12,:]).view(H,W,3)\n",
        "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
        "\n",
        "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
        "    if(flag):\n",
        "        plt.imshow((overlay).numpy()); \n",
        "        plt.axis('off');\n",
        "        plt.show()\n",
        "    return overlay\n",
        "\n",
        "y = 57\n",
        "gray = torch.clamp(imgs_val[1,0,:,y,:].t().flip(0).data.cpu()+4/5,0,7/5)*5/7\n",
        "rgb = overlaySegment(gray,predict_all[1,:,y,:].t().flip(0).float(),label_rgb/255,True)\n",
        "\n",
        "# TODO Visualize multiple images with corresponding overlays\n",
        "# ...\n"
      ],
      "id": "quantitative-messaging",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equipped-knowing"
      },
      "source": [
        "y = 57\n",
        "gray = torch.clamp(imgs_val[5,0,:,y,:].t().flip(0).data.cpu()+4/5,0,7/5)*5/7\n",
        "rgb = overlaySegment(gray,predict_all[5,:,y,:].t().flip(0).float(),label_rgb/255,True)\n",
        "\n"
      ],
      "id": "equipped-knowing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dp6CaBiFGjO"
      },
      "source": [
        "**Bonus** Use a dice as a loss function and inspect the results:"
      ],
      "id": "3dp6CaBiFGjO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jMdVbpeFIZf"
      },
      "source": [
        "# ..."
      ],
      "id": "1jMdVbpeFIZf",
      "execution_count": null,
      "outputs": []
    }
  ]
}