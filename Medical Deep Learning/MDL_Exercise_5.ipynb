{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mdl_exercise5_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63d3b45dd53245739ba045b52e1ddee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37456c2224cf40edaa45391f23f75214",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92edd15049254256881cdada5b746558",
              "IPY_MODEL_5568b8b727e24f9bbdaf5dcebf6cbbf1"
            ]
          }
        },
        "37456c2224cf40edaa45391f23f75214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92edd15049254256881cdada5b746558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ac2265d085d4c7eb396e9fa57a37eb3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 531503671,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 531503671,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71d6a1e120db49f09e51e93d06fab0aa"
          }
        },
        "5568b8b727e24f9bbdaf5dcebf6cbbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0fb6fa390ac4ec8aef03fbcd3907e40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 507M/507M [00:03&lt;00:00, 162MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2d55a0099ee43dfa0443ae098aada26"
          }
        },
        "1ac2265d085d4c7eb396e9fa57a37eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71d6a1e120db49f09e51e93d06fab0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0fb6fa390ac4ec8aef03fbcd3907e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2d55a0099ee43dfa0443ae098aada26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRUWE-KitK2d",
        "outputId": "08e72e95-66ab-4aa7-ec28-99a0043d5aab"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import sys\n",
        "#sys.path.append('/content/drive/My Drive/mdl_exercise5')\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import shutil,gzip\n",
        "from IPython.utils import io\n",
        "import torchvision.models as models\n",
        "\n",
        "#run pip install for pytorch flop counter before first use\n",
        "!pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
        "from thop import profile\n",
        "!pip install -q wget\n",
        "import wget\n",
        "\n",
        "#some functions to count unique parameters and sparsity are provided\n",
        "def countParameters(net):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "    params = sum([p.numel() for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "def countUnique(net):\n",
        "    unique = 0\n",
        "    for m in net.modules():\n",
        "        if(isinstance(m,nn.Conv2d)):\n",
        "            unique += len(np.unique(m.weight.data.cpu().flatten().numpy()))\n",
        "    return unique\n",
        "    #print('#unique',unique)\n",
        "\n",
        "def countSparsity(net):\n",
        "    count_nonzero = 0; count_zero = 0\n",
        "    for m in net.modules():\n",
        "        if(isinstance(m, nn.Conv2d)):\n",
        "            count_nonzero += torch.sum((m.weight.data!=0).float())\n",
        "            count_zero += torch.sum((m.weight.data==0).float())\n",
        "    return count_zero/(count_zero+count_nonzero)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
            "  Cloning https://github.com/Lyken17/pytorch-OpCounter.git to /tmp/pip-req-build-jwremf5l\n",
            "  Running command git clone -q https://github.com/Lyken17/pytorch-OpCounter.git /tmp/pip-req-build-jwremf5l\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from thop==0.0.4-2106210645) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop==0.0.4-2106210645) (3.7.4.3)\n",
            "Building wheels for collected packages: thop\n",
            "  Building wheel for thop (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thop: filename=thop-0.0.4.post2106210645-cp37-none-any.whl size=8739 sha256=bc33d8419b6a3c98c8a65d9057b19025c4ee33a28a1897171ab88e79980a6db0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gd4eb77v/wheels/79/0e/29/2d013ff0d3e36ae48894c11a6a9eecad6bc4789849f5ed802a\n",
            "Successfully built thop\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.0.4.post2106210645\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "0kHfFFKUtcTY",
        "outputId": "c8d1147f-d02f-40a1-aba5-5c0232c6d52c"
      },
      "source": [
        "#loading the patch-based wholeslide histopathology data (uint8) and converting it to torch tensors\n",
        "import os\n",
        "\n",
        "dataset_url = 'https://cloud.imi.uni-luebeck.de/s/sjjiReHqSnokJ2n/download'\n",
        "\n",
        "def get_data(data_url):\n",
        "    filename = './patchCamelyon8c.mat'\n",
        "    if not os.path.exists(filename):\n",
        "        filename = wget.download(data_url)\n",
        "    \n",
        "get_data(dataset_url)\n",
        "\n",
        "\n",
        "data = scipy.io.loadmat('patchCamelyon8c.mat')\n",
        "print(data.keys())\n",
        "\n",
        "img_train = torch.from_numpy(data['img_train'].astype('float32')/255)\n",
        "img_test = torch.from_numpy(data['img_test'].astype('float32')/255)\n",
        "\n",
        "label_train = torch.from_numpy(data['label_train']).long()\n",
        "label_test = torch.from_numpy(data['label_test']).long()\n",
        "\n",
        "plt.imshow(img_test[100,:,:,:].permute(1,2,0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'img_test', 'img_train', 'label_test', 'label_train'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788,
          "referenced_widgets": [
            "63d3b45dd53245739ba045b52e1ddee1",
            "37456c2224cf40edaa45391f23f75214",
            "92edd15049254256881cdada5b746558",
            "5568b8b727e24f9bbdaf5dcebf6cbbf1",
            "1ac2265d085d4c7eb396e9fa57a37eb3",
            "71d6a1e120db49f09e51e93d06fab0aa",
            "b0fb6fa390ac4ec8aef03fbcd3907e40",
            "e2d55a0099ee43dfa0443ae098aada26"
          ]
        },
        "id": "VKHf1__JtM6x",
        "outputId": "3c076b82-d107-4537-c31b-2188f6ef9574"
      },
      "source": [
        "net = torchvision.models.vgg11_bn(pretrained='True')\n",
        "\n",
        "input = torch.randn(128, 3, 48, 48)\n",
        "with io.capture_output() as captured:\n",
        "    #flops, params = profile(net.features, input_size=(128,3,48,48))\n",
        "    flops, params = profile(net.features, inputs=(input, ))\n",
        "print('#GFlops',flops/1e9)\n",
        "\n",
        "print('#features',countParameters(net))\n",
        "\n",
        "# ========== Task 1: ==========\n",
        "# Replace the layer net.avgpool with an adaptive average pool of output size 1x1\n",
        "# Create a new classifier as nn.Sequential with two linear layers (512x256 and 256x2)\n",
        "# including one ReLU and no batch-norm. This reduces the parameter count to 9.4 million.\n",
        "# Fine-tune this network for 16 sub-epochs with batch size = 128, learning rate = 0.0005,\n",
        "# and decay = 0.9 (use only a random quarter of training patches per epoch).\n",
        "# Evaluate the test accuracy (which should be ~94%, random_seed=7).\n",
        "\n",
        "\n",
        "net.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "net.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "print('#features',countParameters(net))\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63d3b45dd53245739ba045b52e1ddee1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=531503671.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#GFlops 44.127092736\n",
            "#features 132868840\n",
            "#features 9357826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9u5Esvn7mG2",
        "outputId": "b8959a96-956a-431c-efd7-7d91e95545a2"
      },
      "source": [
        "#FINETUNE\n",
        "\n",
        "torch.manual_seed(7)\n",
        "\n",
        "nr_epx = 16\n",
        "b_sz_train = 128\n",
        "init_lr = 0.0005\n",
        "decay = 0.9\n",
        "\n",
        "# initialize the optimizer and scheduler\n",
        "optimizer = optim.Adam(net.parameters(), lr=init_lr)#\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay)\n",
        "metric = nn.CrossEntropyLoss()\n",
        "\n",
        "run_loss = np.zeros(nr_epx)/0\n",
        "run_acc = np.zeros(nr_epx)/0\n",
        "\n",
        "\n",
        "for epoch in range(nr_epx):\n",
        "\n",
        "    run_loss[epoch] = 0\n",
        "    run_acc[epoch] = 0\n",
        "    \n",
        "    # generate a random permutation tensor to draw random batches from the given training data\n",
        "    train_perm = torch.randperm(img_train.size(0))\n",
        "    \n",
        "    \n",
        "    # loop over all batches\n",
        "    for i in range(0,img_train.size(0), b_sz_train):\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # draw the current random image batch and the associated class labels \n",
        "        # based on the random permutation tensor\n",
        "        # HINT: don't forget to adapt the input value range\n",
        "        img_batch = img_train[train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "\n",
        "        \n",
        "        \n",
        "        label_batch = label_train[:,train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "        \n",
        "        output = net(img_batch)\n",
        "\n",
        "\n",
        "        loss = metric(output,label_batch[0,:])\n",
        "        accuracy = torch.mean((output.argmax(1)==label_batch).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # TODO: keep track of the accuracy\n",
        "        run_acc[epoch] += accuracy#\n",
        "        \n",
        "    run_acc[epoch] /= (img_train.size(0)/b_sz_train)# \n",
        "    run_loss[epoch] += loss.item()\n",
        "    \n",
        "    print(epoch+1,'/',nr_epx,'loss',run_loss[epoch],'accuracy',run_acc[epoch])\n",
        "    scheduler.step()\n",
        "\n",
        "# SAVE THE TRAINED MODEL\n",
        "torch.save(net.cpu(),'finetuned_vgg.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 / 16 loss 0.31904366612434387 accuracy 0.89288330078125\n",
            "2 / 16 loss 0.12647239863872528 accuracy 0.9391021728515625\n",
            "3 / 16 loss 0.054242655634880066 accuracy 0.961029052734375\n",
            "4 / 16 loss 0.13187704980373383 accuracy 0.9759979248046875\n",
            "5 / 16 loss 0.038841720670461655 accuracy 0.9859161376953125\n",
            "6 / 16 loss 0.03331739827990532 accuracy 0.9910888671875\n",
            "7 / 16 loss 0.015376436524093151 accuracy 0.9936676025390625\n",
            "8 / 16 loss 0.003914546687155962 accuracy 0.9960479736328125\n",
            "9 / 16 loss 0.00494737271219492 accuracy 0.996826171875\n",
            "10 / 16 loss 0.0029841207433491945 accuracy 0.9974517822265625\n",
            "11 / 16 loss 0.006442893296480179 accuracy 0.99822998046875\n",
            "12 / 16 loss 0.009748170152306557 accuracy 0.998138427734375\n",
            "13 / 16 loss 0.00030295041506178677 accuracy 0.998870849609375\n",
            "14 / 16 loss 0.00012421890278346837 accuracy 0.998870849609375\n",
            "15 / 16 loss 2.5866746000247076e-05 accuracy 0.999114990234375\n",
            "16 / 16 loss 0.0032168535981327295 accuracy 0.9991912841796875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpjeBJvSPQqc",
        "outputId": "40e6ba12-f896-45cf-b0d5-ecdd0e44135e"
      },
      "source": [
        "# check the model's accuracy on the test dataset\n",
        "# HINT: don't forget to switch to .eval() mode...\n",
        "# you should achieve test accuracies ~80%\n",
        "\n",
        "net = torch.load('finetuned_vgg.pth')\n",
        "net.eval()\n",
        "net.cuda()\n",
        "\n",
        "test_acc = 0.0\n",
        "\n",
        "# LOOP over the testdata set\n",
        "for i in range(0,img_test.size(0), b_sz_train):\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    # draw the current random image batch and the associated class labels \n",
        "    # based on the random permutation tensor\n",
        "    # HINT: don't forget to adapt the input value range\n",
        "    img_batch = img_train[train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "\n",
        "        \n",
        "        \n",
        "    label_batch = label_train[:,train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "        \n",
        "    output = net(img_batch)\n",
        "\n",
        "    #loss = metric(output,label_batch)\n",
        "    accuracy = torch.mean((output.argmax(1)==label_batch).float())\n",
        "\n",
        "    # TODO: keep track of the accuracy\n",
        "    test_acc += accuracy#\n",
        "    \n",
        "print('Test accuracy:', test_acc/(img_test.size(0)/b_sz_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: tensor(0.9991, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0DMYXwivPPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ccfe62-42f8-4e4f-c566-54271cebc1eb"
      },
      "source": [
        "# ========== Task 2: ==========\n",
        "# Start with the same modified, pre-trained (not fine-tuned) vgg11_bn as before. \n",
        "# Add a sparsity promoting loss (sum of absolute values) with a factor of 0.04\n",
        "# to the weight and bias parameters of  each BatchNorm2d.\n",
        "# Iterate through all modules() and determine the layer type using isinstance\n",
        "# Retrain and save the network. Evaluate its test accuracy (will drop slightly to ~89%)\n",
        "# Write a function that determines a threshold for input/output neurons to be set to zero\n",
        "# (the ones which have been reduced in absolute value using the sparsity constraint).\n",
        "# You can use the function topk (similar to nth_element in C++),\n",
        "# which outputs both the values and indices sorted around a chosen quantile/percentile.\n",
        "# Here we simply use the median to set 50% of values to zero.\n",
        "# When applied correctly (as incoming & outgoing mask) for each Conv2d layer, \n",
        "# it reduces the nonzero parameters by ~75% (the first incoming & last outgoing Conv2d are not masked).\n",
        "# Note that BatchNorm has four tensors and two index masks have to be applied as follows: \n",
        "#     B = A[idx_next,:,:,:][:,idx_prev,:,:]\n",
        "# Now you can replace all Conv2d and BatchNorm2d layers with smaller filters \n",
        "# (and copy their weights) so that we have the following sequence of channels:\n",
        "# 3, 32, 64, (2x)128, (3x)256, 512  \n",
        "# Evaluate the slimmed network (you could observe a slight improvement to ~92%)\n",
        "# and confirm that the required computations are reduced to 12 GFlops\n",
        "\n",
        "net = torchvision.models.vgg11_bn(pretrained='True')\n",
        "net.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "net.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "net.cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(7)\n",
        "\n",
        "nr_epx = 16\n",
        "b_sz_train = 128\n",
        "init_lr = 0.0005\n",
        "decay = 0.9\n",
        "\n",
        "# initialize the optimizer and scheduler\n",
        "optimizer = optim.Adam(net.parameters(), lr=init_lr)#\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay)\n",
        "metric = nn.CrossEntropyLoss()\n",
        "\n",
        "run_loss = np.zeros(nr_epx)/0\n",
        "run_acc = np.zeros(nr_epx)/0\n",
        "\n",
        "\n",
        "for epoch in range(nr_epx):\n",
        "\n",
        "    run_loss[epoch] = 0\n",
        "    run_acc[epoch] = 0\n",
        "    \n",
        "    # generate a random permutation tensor to draw random batches from the given training data\n",
        "    train_perm = torch.randperm(img_train.size(0))\n",
        "    \n",
        "    \n",
        "    # loop over all batches\n",
        "    for i in range(0,img_train.size(0), b_sz_train):\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # draw the current random image batch and the associated class labels \n",
        "        # based on the random permutation tensor\n",
        "        # HINT: don't forget to adapt the input value range\n",
        "        img_batch = img_train[train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "\n",
        "        \n",
        "        \n",
        "        label_batch = label_train[:,train_perm[max(0, i-b_sz_train):max(i, b_sz_train)]].cuda()# TODO\n",
        "        \n",
        "        output = net(img_batch)\n",
        "\n",
        "\n",
        "        loss = metric(output,label_batch[0,:])\n",
        "        accuracy = torch.mean((output.argmax(1)==label_batch).float())\n",
        "\n",
        "        s_loss = 0\n",
        "        for module in net.modules():\n",
        "          if isinstance(module, nn.BatchNorm2d):\n",
        "            s_loss += 0.04*module.weight.abs().sum()\n",
        "            s_loss += 0.04*module.bias.abs().sum()\n",
        "\n",
        "        loss += s_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # TODO: keep track of the accuracy\n",
        "        run_acc[epoch] += accuracy#\n",
        "        \n",
        "    run_acc[epoch] /= (img_train.size(0)/b_sz_train)# \n",
        "    run_loss[epoch] += loss.item()\n",
        "    \n",
        "    print(epoch+1,'/',nr_epx,'loss',run_loss[epoch],'accuracy',run_acc[epoch])\n",
        "    scheduler.step()\n",
        "\n",
        "# SAVE THE TRAINED MODEL\n",
        "torch.save(net.cpu(),'sparsity_loss_vgg.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 / 16 loss 32.06831359863281 accuracy 0.8822479248046875\n",
            "2 / 16 loss 14.059919357299805 accuracy 0.8907928466796875\n",
            "3 / 16 loss 5.613494873046875 accuracy 0.89697265625\n",
            "4 / 16 loss 1.397934079170227 accuracy 0.90570068359375\n",
            "5 / 16 loss 0.532512903213501 accuracy 0.913482666015625\n",
            "6 / 16 loss 0.41881895065307617 accuracy 0.919769287109375\n",
            "7 / 16 loss 0.391041100025177 accuracy 0.92333984375\n",
            "8 / 16 loss 0.3393602967262268 accuracy 0.927886962890625\n",
            "9 / 16 loss 0.28034019470214844 accuracy 0.934051513671875\n",
            "10 / 16 loss 0.40048664808273315 accuracy 0.9373779296875\n",
            "11 / 16 loss 0.29310187697410583 accuracy 0.940216064453125\n",
            "12 / 16 loss 0.22089475393295288 accuracy 0.9452667236328125\n",
            "13 / 16 loss 0.3014027178287506 accuracy 0.9478759765625\n",
            "14 / 16 loss 0.2410624623298645 accuracy 0.951873779296875\n",
            "15 / 16 loss 0.20358608663082123 accuracy 0.9541015625\n",
            "16 / 16 loss 0.21352095901966095 accuracy 0.957244873046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEAryxAeNCPp"
      },
      "source": [
        "net = torch.load('sparsity_loss_vgg.pth')\n",
        "\n",
        "for module in net.modules():\n",
        "  if isinstance(module, nn.Conv2d):\n",
        "    w = module.weight.data.clone().detach()\n",
        "    b = module.bias.data.clone().detach()\n",
        "    #m = module.running_mean.data.clone().detach()\n",
        "    #v = module.running_var.data.clone().detach()\n",
        "    vals, indxs = torch.topk(w,int(w.shape[0]/2),dim=0)\n",
        "\n",
        "    w[indxs] = 0\n",
        "    module.weight.data = w\n",
        "    b[indxs] = 0\n",
        "    module.bias.data = w\n",
        "    #m[indxs] = 0\n",
        "    #module.running_mean.data = w\n",
        "    #v[indxs] = 0\n",
        "    #module.running_var.data = w\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Q4ay90rCv3gi",
        "outputId": "5db97a66-8f71-4e5a-ec38-44bf01601941"
      },
      "source": [
        "# ========== Task 3: ==========\n",
        "# Start with the same modified, pre-trained (not fine-tuned) vgg11_bn as before. \n",
        "# Finally we want to explore, how the memory storage can be reduced with little loss. \n",
        "# Here a ternary weight approximation will be used for which we first estimate\n",
        "# a per-channel delta for each weight in Conv2d given the rule-of-thumb below. \n",
        "# Tip: after calculating the absolute values the mean has to be computed over all but the 0-th dimension.\n",
        "# The obtained ternary weights have lost their magnitude, therefore the parameter \n",
        "# alpha (again per-channel) is computed and multiplied with the weight tensor\n",
        "\n",
        "# template for function in task 3\n",
        "def approx_weights(w_in, flag=True):\n",
        "    if(flag): #we want to use ternary weight approximation\n",
        "        with torch.no_grad():\n",
        "            a,b,c,d = w_in.size()\n",
        "            delta = 0.7 * torch.ones([a,1,1,1])#*.. there should be #a different delta values (viewed as 4D tensor)\n",
        "            alpha = 1 * torch.ones([a,1])#.. there should be #a different alpha values (~)\n",
        "            \n",
        "            w_out = delta + alpha * w_in #.. determine correct formula for output weights\n",
        "    else:\n",
        "        w_out = w_in\n",
        "    return w_out\n",
        "\n",
        "\n",
        "net = torchvision.models.vgg11_bn(pretrained='True')\n",
        "net.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "net.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "net.cuda()\n",
        "\n",
        "# check the number of unique values before/after ternary approximation \n",
        "w_in = net.features[4].weight.clone().detach()\n",
        "w_approx = approx_weights(w_in,True)\n",
        "print('#unique',len(np.unique(w_in.data.cpu().flatten().numpy())))\n",
        "print('#unique',len(np.unique(w_approx.data.cpu().flatten().numpy())))\n",
        "\n",
        "# For a 128 x 64 x 3 x 3 kernel the number of unique entries is reduced from \n",
        "# more than 70 thousand to just 257 (2 x 128 + 1).\n",
        "# To effectively train a network with weight quantisation, it is important to only use the\n",
        "# ternary weights during forward/backward path, but update their gradients in full precision\n",
        "# Implement a loop that stores full precision weights in a list of tensors and replaces\n",
        "# the .data values with their approximation just before calling the forward pass (and zero_grad)\n",
        "# Reassign these backup copies after loss.backward() and before optimizer.step(). \n",
        "# Tip: you could use .pop(0) to (iteratively) access and remove the first object of a list.\n",
        "# Retrain your network and take care to perform the weight quantisation the same way \n",
        "# during test evaluation. The test accuracy should be around 85-90% during the epochs.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d44bf384ebcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# check the number of unique values before/after ternary approximation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mw_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mw_approx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#unique'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#unique'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_approx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d44bf384ebcc>\u001b[0m in \u001b[0;36mapprox_weights\u001b[0;34m(w_in, flag)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.. there should be #a different alpha values (~)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mw_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_in\u001b[0m \u001b[0;31m#.. determine correct formula for output weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mw_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MERtcSlOv_1w"
      },
      "source": [
        "#Note: you can calculate the compressed model size (37MB to 2.8MB) with the following code snippet:\n",
        "torch.save(net.features.cpu().state_dict(),model_name+'_feat.pth')\n",
        "with open(model_name+'_feat.pth', 'rb') as f_in, gzip.open(model_name+'_feat.pth.gz', 'wb') as f_out:\n",
        "   shutil.copyfileobj(f_in, f_out)\n",
        "os.path.getsize(model_name+'_feat.pth.gz')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}