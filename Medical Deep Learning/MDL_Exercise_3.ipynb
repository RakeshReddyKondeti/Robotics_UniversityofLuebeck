{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Kopie von MDL_Exercise03_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkcnBlOokZpS"
      },
      "source": [
        "# Medical Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ZR-gga246T"
      },
      "source": [
        "## Exercise 3: Learning-based multi-modal 3D registration (20 points)\n",
        "\n",
        "### (Hanna Siebert, Marian Himstedt & Matthias Heinrich)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkt6rNUYUfez"
      },
      "source": [
        "**Please upload your commented solution (`mdl_exercise3.ipynb`) to Moodle by Tuesday 25.5. 23:59.** Check whether the code can be executed after restarting the kernel and running through all cells sequentially.\n",
        "\n",
        "The aim of this exercise is to introduce you to deep-learning based image registration and also explore mutual information as a metric to supervise the training of multi-modal feature networks. The method we want to implement comprises three parts: \n",
        "\n",
        "1.   A global mutual information loss function\n",
        "2.   A correlation layer to robustly estimate large rigid transformations\n",
        "3.   A compact 3D CNN network (with some modality specific and some shared layers) to predict features suitable for multi-modal CT/MR registration\n",
        "\n",
        "**Provided functions and data loading**\n",
        "\n",
        "The following cells provide the fundamental for loading and augmenting the data. Also you are given a number of functions for the subsequent registration tasks. You are invited to read and understand the code if you are interested.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g56qYcdnGppb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd007522-e72a-4983-a049-a3b879c43618"
      },
      "source": [
        "# Download train and test data\n",
        "!wget https://cloud.imi.uni-luebeck.de/s/76KJ7RBqpsdjSbw/download -O mdl3_masks.npz\n",
        "!wget https://cloud.imi.uni-luebeck.de/s/yAZNkTBRGoeePZa/download -O mdl3_imgs.pth\n",
        "\n",
        "# Download feature data for testing task 1 & 2\n",
        "!wget https://cloud.imi.uni-luebeck.de/s/nSixsneJ6fDbfBB/download -O mdl3_exercise_task12.pth\n",
        "\n",
        "# Download an additional python file providing utility functions\n",
        "\n",
        "!wget https://cloud.imi.uni-luebeck.de/s/X8A8Dixgj62Qwtf/download -O mdl_exercise3_utils.py\n",
        "\n",
        "from mdl_exercise3_utils import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-21 20:25:36--  https://cloud.imi.uni-luebeck.de/s/76KJ7RBqpsdjSbw/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1638735 (1.6M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_masks.npz’\n",
            "\n",
            "mdl3_masks.npz      100%[===================>]   1.56M  2.11MB/s    in 0.7s    \n",
            "\n",
            "2021-05-21 20:25:38 (2.11 MB/s) - ‘mdl3_masks.npz’ saved [1638735/1638735]\n",
            "\n",
            "--2021-05-21 20:25:39--  https://cloud.imi.uni-luebeck.de/s/yAZNkTBRGoeePZa/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 188744833 (180M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_imgs.pth’\n",
            "\n",
            "mdl3_imgs.pth       100%[===================>] 180.00M  18.9MB/s    in 10s     \n",
            "\n",
            "2021-05-21 20:25:50 (17.4 MB/s) - ‘mdl3_imgs.pth’ saved [188744833/188744833]\n",
            "\n",
            "--2021-05-21 20:25:50--  https://cloud.imi.uni-luebeck.de/s/nSixsneJ6fDbfBB/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64883639 (62M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_exercise_task12.pth’\n",
            "\n",
            "mdl3_exercise_task1 100%[===================>]  61.88M  19.5MB/s    in 4.0s    \n",
            "\n",
            "2021-05-21 20:25:55 (15.5 MB/s) - ‘mdl3_exercise_task12.pth’ saved [64883639/64883639]\n",
            "\n",
            "--2021-05-21 20:25:55--  https://cloud.imi.uni-luebeck.de/s/X8A8Dixgj62Qwtf/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3249 (3.2K) [text/x-python]\n",
            "Saving to: ‘mdl_exercise3_utils.py’\n",
            "\n",
            "mdl_exercise3_utils 100%[===================>]   3.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-21 20:25:56 (735 MB/s) - ‘mdl_exercise3_utils.py’ saved [3249/3249]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBK2xklWPXSQ"
      },
      "source": [
        "# some parameters\n",
        "\n",
        "grid_step = 12\n",
        "disp_radius = 4\n",
        "disp_step = 5\n",
        "beta = 25\n",
        "\n",
        "W = D = 192\n",
        "H = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxu3jejIW3u"
      },
      "source": [
        "def least_trimmed_rigid(fixed_pts, moving_pts, iter=5):\n",
        "    idx = torch.arange(fixed_pts.shape[0]).to(fixed_pts.device)\n",
        "    for i in range(iter):\n",
        "        x = find_rigid_3d(fixed_pts[idx,:], moving_pts[idx,:]).t()\n",
        "        residual = torch.sqrt(torch.sum(torch.pow(moving_pts - torch.mm(fixed_pts, x), 2), 1))\n",
        "        _, idx = torch.topk(residual, fixed_pts.shape[0]//2, largest=False)\n",
        "    return x.t().to(fixed_pts.dtype)\n",
        "\n",
        "def find_rigid_3d(x, y):\n",
        "    x_mean = x[:, :3].mean(0)\n",
        "    y_mean = y[:, :3].mean(0)\n",
        "    u, s, v = torch.svd(torch.matmul((x[:, :3]-x_mean).t(), (y[:, :3]-y_mean)).float())\n",
        "    m = torch.eye(v.shape[0], v.shape[0]).to(x.device)\n",
        "    m[-1,-1] = torch.det(torch.matmul(v, u.t()).float())\n",
        "    rotation = torch.matmul(torch.matmul(v, m), u.t())\n",
        "    translation = y_mean - torch.matmul(rotation, x_mean)\n",
        "    T = torch.eye(4).to(x.device)\n",
        "    T[:3,:3] = rotation\n",
        "    T[:3, 3] = translation\n",
        "    return T\n",
        "\n",
        "def generate_random_rigid_3d(strength=.3):\n",
        "    x = torch.randn(12,3).to(device)\n",
        "    y = x + strength*torch.randn(12,3).to(device)\n",
        "    return find_rigid_3d(x, y)\n",
        "\n",
        "disp = torch.stack(torch.meshgrid(torch.arange(- disp_step * disp_radius, disp_step * disp_radius + 1, disp_step),\\\n",
        "                                      torch.arange(- disp_step * disp_radius , disp_step * disp_radius  + 1, disp_step),\\\n",
        "                                      torch.arange(- disp_step * disp_radius , disp_step * disp_radius  + 1, disp_step))).permute(1, 2, 3, 0).contiguous().view(1, 1, -1, 1, 3).float()\n",
        "\n",
        "disp = (disp.flip(-1) * 2 / (torch.tensor([W, H, D]) - 1))#.to(dtype).to(device)\n",
        "    \n",
        "disp_width = disp_radius * 2 + 1\n",
        "\n",
        "#finding 50% best matches and computing soft-correspondences is provided as \"robust_rigid_fit\"\n",
        "#in initial notebook, it receives the ssd_cost tensor N x 729 from Task 2 and returns a 4x4 matrix R\n",
        "\n",
        "def robust_rigid_fit(ssd_cost,kpts_fixed,feat_fix):\n",
        "    ssd_cost = ssd_cost.view(1,-1,(disp_radius*2+1)**3)\n",
        "    kpts_fixed = kpts_fixed.view(1,-1,3)\n",
        "    #mask_fix, feat_fix, feat_mov):#, grid_step, disp_radius, disp_step, beta=15):\n",
        "    #use predefined set of displacements\n",
        "    disp1 = disp.to(ssd_cost.device).to(ssd_cost.dtype)\n",
        "    \n",
        "    #remove 50% least reliable control points based on the minimum cost of their respective values\n",
        "    ssd_val, ssd_idx = torch.min(ssd_cost.squeeze(), 1)\n",
        "    idx_best = torch.sort(ssd_val, dim=0, descending=False)[1][:kpts_fixed.shape[1]//2]\n",
        "    #compute a weighted soft correspondence (displacement)\n",
        "    #this step is crucial to keep the loss differentiable!\n",
        "    disp_best = torch.sum(torch.softmax(-beta*ssd_cost.squeeze(0).unsqueeze(2),1) * disp1.view(1, -1, 3), 1)\n",
        "    disp_best = disp_best[idx_best,:]\n",
        "    \n",
        "    #compute absolute coordinates for coresspondences and run least trimmed squares fitting\n",
        "    fixed_pts = torch.cat((kpts_fixed[0,idx_best,:], torch.ones(idx_best.size(0),1).to(feat_fix.device).to(feat_fix.dtype)),1)\n",
        "    moving_pts = torch.cat((kpts_fixed[0,idx_best,:] + disp_best, torch.ones(idx_best.size(0),1).to(feat_fix.device).to(feat_fix.dtype)),1)\n",
        "    with torch.cuda.amp.autocast(enabled=False): #SVD is not available/stable with FP16\n",
        "        R = least_trimmed_rigid(fixed_pts.float(), moving_pts.float())\n",
        "    return R[:3,:4].unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EozAaztrACST"
      },
      "source": [
        "# Run all required imports\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z37N0EVQACSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877b58ba-83c4-47f9-d2bb-41aab1000d7c"
      },
      "source": [
        "#load data and created eight augmented versions each (64 CT/MR pairs in total)\n",
        "\n",
        "mdl3_imgs = torch.load('mdl3_imgs.pth')\n",
        "mdl3_masks = np.load('mdl3_masks.npz')\n",
        "\n",
        "def load_case(case):\n",
        "    img_fix = mdl3_imgs['mdl3_img_fix'][case].float().cpu()\n",
        "    img_mov = mdl3_imgs['mdl3_img_mov'][case].float().cpu()\n",
        "    mask_fix = torch.from_numpy(mdl3_masks['mdl3_mask_fix'][case]).cpu()\n",
        "    mask_mov = torch.from_numpy(mdl3_masks['mdl3_mask_mov'][case]).cpu()\n",
        "    seg_fix = torch.from_numpy(mdl3_masks['mdl3_seg_fix'][case]).cpu().long()\n",
        "    seg_mov = torch.from_numpy(mdl3_masks['mdl3_seg_mov'][case]).cpu().long()\n",
        "    return img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov\n",
        "\n",
        "\n",
        "TRAIN_CASES = torch.arange(8) \n",
        "\n",
        "imgs_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).float().pin_memory()\n",
        "imgs_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).float().pin_memory()\n",
        "segs_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).int().pin_memory()\n",
        "segs_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).int().pin_memory()\n",
        "masks_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).bool().pin_memory()\n",
        "masks_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).bool().pin_memory()\n",
        "for i, case in enumerate(TRAIN_CASES):\n",
        "    print('process case', i)\n",
        "    img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case)\n",
        "    device = img_fix.device\n",
        "    img_fix = img_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    img_mov = img_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    seg_fix = seg_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    seg_mov = seg_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    mask_fix = mask_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    mask_mov = mask_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    imgs_mov_train[i:i+1] = img_mov\n",
        "    segs_mov_train[i:i+1] = seg_mov\n",
        "    masks_mov_train[i:i+1] = mask_mov\n",
        "    for j in range(8):\n",
        "        with torch.no_grad():\n",
        "            R = generate_random_rigid_3d()\n",
        "            grid = F.affine_grid(R[:3,:4].unsqueeze(0).cuda(), (1,1,D,H,W))\n",
        "            img_fix_ = F.grid_sample(img_fix.cuda(), grid, padding_mode='border')\n",
        "            seg_fix_ = F.grid_sample(F.one_hot(seg_fix[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float().cuda(), grid).argmax(1, keepdim=True).int()\n",
        "            mask_fix_ = F.grid_sample(mask_fix.float().cuda(), grid)>0.5\n",
        "\n",
        "            imgs_fix_train[i:i+1, j:j+1] = img_fix_.cpu()\n",
        "            segs_fix_train[i:i+1, j:j+1] = seg_fix_.cpu()\n",
        "            masks_fix_train[i:i+1, j:j+1] = mask_fix_.cpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process case 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3891: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3829: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "process case 1\n",
            "process case 2\n",
            "process case 3\n",
            "process case 4\n",
            "process case 5\n",
            "process case 6\n",
            "process case 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq8r0c9aHVOm"
      },
      "source": [
        "### Task 1 (9 points): Computation of (joint) histograms and mutual information\n",
        "\n",
        "\n",
        " The values should be sampled of the (fixed) mask to exclude background locations\n",
        "\n",
        "✔ Define a range for the histogram bins with linspace with 64 steps and the minimum and maximum value of the respective image.\n",
        "\n",
        "✔ Compute a Parzen window weighting with σ=0.015 as exp( - $(value - bin)^2$ / (2 ⋅ $σ^2$)), after this step you should have two 2D tensors of size 64 x N (where N is the number of pixels) \n",
        "\n",
        "✔ Calculate the marginal (individual) and the joint histogram by summing/averaging over the pixels and dividing the resulting vector by its sum.  For the joint histogram the pairwise sums are implicitly obtained using a matrix multiplication of fixed and transposed moving histograms.\n",
        "\n",
        "✔ Use E = - ∑ p ⋅ log2(p + ε) to compute entropy and -($E_{fix}$ + $E_{mov}$ - $E_{joint}$) as MI loss, ε = $10^{-6}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1m9AIk0ACSb"
      },
      "source": [
        "def mutual_inf(mask_fix,img_fix,img_mov):\n",
        "    sigma = 0.015\n",
        "\n",
        "    # TODO: draw samples from mask_fix\n",
        "    img_fix_flattened = img_fix.view(-1)[mask_fix.view(-1) > 0]\n",
        "    img_mov_flattened = img_mov.view(-1)[mask_fix.view(-1) > 0]\n",
        "    random_indices = np.random.randint(torch.numel(img_fix_flattened), size=int(torch.numel(img_fix_flattened)*0.2))\n",
        "\n",
        "    img_fix_sampled = img_fix_flattened[random_indices]\n",
        "    img_mov_sampled = img_mov_flattened[random_indices]\n",
        "\n",
        "\n",
        "    # TODO: define bins\n",
        "    minimum_fix = torch.min(img_fix_sampled)\n",
        "    maximum_fix = torch.max(img_fix_sampled)\n",
        "    bins_fix = torch.linspace(minimum_fix, maximum_fix, 64).cuda()\n",
        "\n",
        "    minimum_mov = torch.min(img_mov_sampled)\n",
        "    maximum_mov = torch.max(img_mov_sampled)\n",
        "    bins_mov = torch.linspace(minimum_mov, maximum_mov, 64).cuda()\n",
        "\n",
        "    # TODO: estimate histograms\n",
        "    rep_fix = img_fix_sampled.unsqueeze(1).repeat(1, bins_fix.size()[0]).cuda()\n",
        "    rep_mov = img_mov_sampled.unsqueeze(1).repeat(1, bins_mov.size()[0]).cuda()\n",
        "\n",
        "    hist_fix = torch.exp(-(rep_fix - bins_fix)**2 / (2*sigma**2)).t()\n",
        "    hist_mov = torch.exp(-(rep_mov - bins_mov)**2 / (2*sigma**2)).t()\n",
        "\n",
        "    hist_fix_sum = hist_fix.sum(dim=1).cuda()\n",
        "    hist_mov_sum = hist_mov.sum(dim=1).cuda()\n",
        "\n",
        "    hist_fix_marg = hist_fix_sum/hist_fix_sum.sum().cuda()\n",
        "    hist_mov_marg = hist_mov_sum/hist_mov_sum.sum().cuda()\n",
        "\n",
        "    joint_hist = torch.matmul(hist_fix, hist_mov.t()).cuda()\n",
        "    joint_hist = joint_hist/joint_hist.sum().cuda()\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: estimate entropies\n",
        "    E_fix = torch.matmul(-hist_fix_marg,torch.log2(hist_fix_marg+0.000001)).cuda()# ...\n",
        "    E_mov = torch.matmul(-hist_mov_marg,torch.log2(hist_mov_marg+0.000001)).cuda()# ...\n",
        "    j_h_flattened = joint_hist.view(-1)\n",
        "    E_joint = torch.matmul(-j_h_flattened,torch.log2(j_h_flattened+0.000001)).cuda()\n",
        "    return -(E_fix+E_mov-E_joint)                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUukq6x0L1kd"
      },
      "source": [
        "### Task 2 (5 points): Correlation layer  \n",
        "\n",
        "✔ Define a grid of control points using affine_grid with a spacing of 12 voxels and range of .925 (to exclude points near the image boundaries). Again sample the values of the (fixed) mask.  \n",
        "\n",
        "✔ Create an empty tensor (with same type and device as the fixed features) of size N x 729 (N = number of control points, 729 = 93 displacements), use 32 chunks for unrolling and compute the (dis)similarity as follows: sample the fixed features at (the current subset of) grid points and the moving feature tensor at the combined (added) coordinates of absolute grid points and relative displacements yielding a C x N/32 x 729 tensor. Square and sum over the channel dimension (C).    \n",
        "\n",
        "✔ The resulting SSD tensor should be fed to the provided function robust_rigid_fit, which searches for the most probable correspondences, by filtering out potentially erroneous ones based on their similarity score and the residual of a globally rigid least-square fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXQDWp0DACSY"
      },
      "source": [
        "# Task2 correlation layer\n",
        "def correlation(mask_fix, feat_fixed, feat_moving):\n",
        "    unroll_factor=32 \n",
        "    voxel_spacing = 12\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    affine_matrix = (torch.eye(3, 4).unsqueeze(0)).cuda()\n",
        "    kpts_fixed = F.affine_grid((0.925)*(affine_matrix), torch.Size((1, 1, 192//12 , 160//12, 192//12))).float().cuda()\n",
        "\n",
        "    sample_mask = F.grid_sample(mask_fix, kpts_fixed)\n",
        "\n",
        "    N = (192//12)*(160//12)*(192//12)\n",
        "    empty_tensor = torch.empty((N,729)).cuda()\n",
        "\n",
        "    sample_fixed = F.grid_sample(feat_fixed.float(), kpts_fixed).reshape(64,-1)\n",
        "\n",
        "    #N_ = int(N/unroll_factor)\n",
        "\n",
        "    #outp = torch.empty((1, 1, 729)).cuda()\n",
        "\n",
        "    #for idx in range(unroll_factor):\n",
        "    #  empt = torch.empty((1, N_, 1, 1, 3)).cuda()\n",
        "    #  rel_coords = (disp.cuda() + empt).cuda()\n",
        "\n",
        "    #  sample_mov = F.grid_sample(feat_moving.float(), rel_coords)#.reshape(64,N_,-1)\n",
        "    #  outp = torch.cat((outp, sample_mov[:,0,:,:,:].squeeze(3)),1)\n",
        "\n",
        "    #print(outp.size(), sample_fixed.size())\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #We understand what we need to do:\n",
        "    #Add the offsets from \"disp\" to each voxel of \"feat_mov\" and do grid_sample, similar to line 19.\n",
        "    #Also chunk the tensor and process one chunk at a time, such that there is no memory issues.\n",
        "\n",
        "\n",
        "    new_tensor = torch.empty((32, 16, 13, 16, 3)).cuda()\n",
        "\n",
        "    for idx in range(math.floor(729/32)):\n",
        "      for a in range(32):\n",
        "        for b in range(16):\n",
        "          for c in range(13):\n",
        "            for d in range(16):\n",
        "              new_tensor[a, b,c,d,:] = disp[:,:,a*(idx+1),:,:].squeeze(1).squeeze(1).squeeze(1)\n",
        "      \n",
        "\n",
        "      new_featMov = torch.empty((32, 64, 48, 40, 48)).cuda()\n",
        "      for a in range(32):\n",
        "        new_featMov[a, :,:,:,:] = feat_moving\n",
        "\n",
        "      sample_mov = F.grid_sample(new_featMov.float(), new_tensor).reshape(64,N,-1)\n",
        "\n",
        "    print(feat_mov.size(), kpts_fixed.size(), sample_fixed.size(), empty_tensor.size())\n",
        "\n",
        "    print(sample_mov - sample_fixed.unsqueeze(2))\n",
        "\n",
        "\n",
        "    ssd = torch.sum(torch.pow(sample_mov - sample_fixed.unsqueeze(2),2),0)\n",
        "    \n",
        "    return ssd,kpts_fixed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752YQwn2Lkjn"
      },
      "source": [
        "**Optional**: The following code enables to verify your solutions for tasks 1 & 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uko1LggQLwOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d0b5a4-9096-47d0-f66c-7d65fd9b1695"
      },
      "source": [
        "#test for task1 and task2\n",
        "\n",
        "task12 = torch.load('mdl3_exercise_task12.pth')\n",
        "\n",
        "print(task12.keys())\n",
        "with torch.no_grad():\n",
        "    mask_fix = task12['mask_fix'].float().cuda()\n",
        "    img_mov = task12['img_mov'].float().cuda()\n",
        "    #call mutual information before transform\n",
        "    loss0 = mutual_inf(mask_fix,task12['img_fix'].float().cuda()[0,0],img_mov[0,0])\n",
        "    print('mi before',loss0)\n",
        "\n",
        "    with torch.cuda.amp.autocast(enabled=True):\n",
        "            #call your own implementation of correlation layer\n",
        "            feat_fix = task12['feat_fix'].half().cuda()\n",
        "            feat_mov = task12['feat_mov'].half().cuda()\n",
        "            cost,kpts_fixed = correlation(mask_fix,feat_fix,feat_mov)\n",
        "            \n",
        "            #provided function for robust fitting\n",
        "            R = robust_rigid_fit(cost.cuda(),kpts_fixed.cuda(),feat_fix)\n",
        "            \n",
        "    #mutual information requires 32bit precision\n",
        "    grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "    img_warped = F.grid_sample(img_mov,grid.float(),mode='bilinear')\n",
        "    loss1 = mutual_inf(mask_fix,task12['img_fix'].float().cuda()[0,0],img_mov)\n",
        "    print('mi after',loss1)  \n",
        "    seg_fix = task12['seg_fix'].float().cuda()\n",
        "    seg_mov = task12['seg_mov'].float().cuda()\n",
        "\n",
        "    seg_mov_warped = F.grid_sample(seg_mov.float(), grid, mode='nearest')\n",
        "\n",
        "    d0 = dice_coeff(seg_fix.cpu(),seg_mov.cpu(),5) \n",
        "    print(d0)\n",
        "    print('mean dice before: ',d0.mean().item())\n",
        "    d1 = dice_coeff(seg_fix.cpu(),seg_mov_warped.cpu(),5)\n",
        "    print(d1)\n",
        "    print('mean dice after: ', d1.mean().item())\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['feat_fix', 'feat_mov', 'seg_mov', 'mask_fix', 'R', 'seg_fix', 'img_fix', 'img_mov'])\n",
            "mi before tensor(-0.1097, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3891: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3829: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 48, 40, 48]) torch.Size([1, 16, 13, 16, 3]) torch.Size([64, 3328]) torch.Size([3328, 729])\n",
            "tensor([[[ 0.0055,  0.0055,  0.0055,  ...,  0.0055,  0.0055,  0.0055],\n",
            "         [ 0.0054,  0.0054,  0.0054,  ...,  0.0054,  0.0054,  0.0054],\n",
            "         [ 0.0054,  0.0054,  0.0054,  ...,  0.0054,  0.0054,  0.0054],\n",
            "         ...,\n",
            "         [ 0.0043,  0.0043,  0.0043,  ...,  0.0043,  0.0043,  0.0043],\n",
            "         [-0.0172, -0.0172, -0.0172,  ..., -0.0172, -0.0172, -0.0172],\n",
            "         [-0.0225, -0.0225, -0.0225,  ..., -0.0225, -0.0225, -0.0225]],\n",
            "\n",
            "        [[ 0.2365,  0.2365,  0.2365,  ...,  0.2365,  0.2365,  0.2365],\n",
            "         [ 0.2365,  0.2365,  0.2365,  ...,  0.2365,  0.2365,  0.2365],\n",
            "         [ 0.2365,  0.2365,  0.2365,  ...,  0.2365,  0.2365,  0.2365],\n",
            "         ...,\n",
            "         [ 0.1517,  0.1517,  0.1517,  ...,  0.1517,  0.1517,  0.1517],\n",
            "         [ 0.1295,  0.1295,  0.1295,  ...,  0.1295,  0.1295,  0.1295],\n",
            "         [ 0.0978,  0.0978,  0.0978,  ...,  0.0978,  0.0978,  0.0978]],\n",
            "\n",
            "        [[-0.2311, -0.2311, -0.2311,  ..., -0.2311, -0.2311, -0.2311],\n",
            "         [-0.2273, -0.2273, -0.2273,  ..., -0.2273, -0.2273, -0.2273],\n",
            "         [-0.2160, -0.2160, -0.2160,  ..., -0.2160, -0.2160, -0.2160],\n",
            "         ...,\n",
            "         [ 0.2282,  0.2282,  0.2282,  ...,  0.2282,  0.2282,  0.2282],\n",
            "         [ 0.2289,  0.2289,  0.2289,  ...,  0.2289,  0.2289,  0.2289],\n",
            "         [ 0.2294,  0.2294,  0.2294,  ...,  0.2294,  0.2294,  0.2294]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0728,  0.0728,  0.0728,  ...,  0.0728,  0.0728,  0.0728],\n",
            "         [ 0.0644,  0.0644,  0.0644,  ...,  0.0644,  0.0644,  0.0644],\n",
            "         [ 0.0618,  0.0618,  0.0618,  ...,  0.0618,  0.0618,  0.0618],\n",
            "         ...,\n",
            "         [-0.2066, -0.2066, -0.2066,  ..., -0.2066, -0.2066, -0.2066],\n",
            "         [-0.1894, -0.1894, -0.1894,  ..., -0.1894, -0.1894, -0.1894],\n",
            "         [-0.1599, -0.1599, -0.1599,  ..., -0.1599, -0.1599, -0.1599]],\n",
            "\n",
            "        [[-0.0327, -0.0327, -0.0327,  ..., -0.0327, -0.0327, -0.0327],\n",
            "         [-0.0413, -0.0413, -0.0413,  ..., -0.0413, -0.0413, -0.0413],\n",
            "         [-0.0494, -0.0494, -0.0494,  ..., -0.0494, -0.0494, -0.0494],\n",
            "         ...,\n",
            "         [ 0.2768,  0.2768,  0.2768,  ...,  0.2768,  0.2768,  0.2768],\n",
            "         [ 0.2273,  0.2273,  0.2273,  ...,  0.2273,  0.2273,  0.2273],\n",
            "         [ 0.1921,  0.1921,  0.1921,  ...,  0.1921,  0.1921,  0.1921]],\n",
            "\n",
            "        [[ 0.2148,  0.2148,  0.2148,  ...,  0.2148,  0.2148,  0.2148],\n",
            "         [ 0.2149,  0.2149,  0.2149,  ...,  0.2149,  0.2149,  0.2149],\n",
            "         [ 0.2149,  0.2149,  0.2149,  ...,  0.2149,  0.2149,  0.2149],\n",
            "         ...,\n",
            "         [ 0.0147,  0.0147,  0.0147,  ...,  0.0147,  0.0147,  0.0147],\n",
            "         [ 0.0051,  0.0051,  0.0051,  ...,  0.0051,  0.0051,  0.0051],\n",
            "         [-0.0093, -0.0093, -0.0093,  ..., -0.0093, -0.0093, -0.0093]]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-f0b449242849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#provided function for robust fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobust_rigid_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkpts_fixed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_fix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#mutual information requires 32bit precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3bd32c7adbc5>\u001b[0m in \u001b[0;36mrobust_rigid_fit\u001b[0;34m(ssd_cost, kpts_fixed, feat_fix)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrobust_rigid_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssd_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkpts_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_fix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mssd_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp_radius\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mkpts_fixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpts_fixed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#mask_fix, feat_fix, feat_mov):#, grid_step, disp_radius, disp_step, beta=15):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, -1, 729]' is invalid for input of size 106496"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mrYTfeFMfhH"
      },
      "source": [
        "### Task 3 (6 points): Define network and train CNNs with mutual information\n",
        "\n",
        "✔ The modality specific modules should both receive a single-channel input and start with 16 feature maps that are doubled to 32 in the second block. All convolutions should be 3D with 3x3x3 kernels and padding=1, followed by InstanceNorm3d and a LeakyReLU. The second block should have a stride=2. \n",
        "\n",
        "✔ The shared sub-network should receive the 32-channel feature maps and comprise three blocks of the same pattern as above: doubling of channels and stride=2 in second block. This yields 64-channel feature that will be squeezed into a range of 0 to 1 using a sigmoid. These feature tensors (for fixed = MRI and moving = CT) will be the input of your correlation layer from Task 2. \n",
        "   \n",
        "✔ The resulting SSD tensor is again fed into robust_rigid_fit, which return a 1x3x4 matrix to obtain a deformation grid that will be applied to the moving scan. Afterwards the mutual information can be computed and minimised as loss. The training loop is pre-defined, we use a batch-size of 1 (hence InstanceNorm) and train for 30 epochs (240 iterations). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8dBZJtkACSZ"
      },
      "source": [
        "class ModalityNet(nn.Module):\n",
        "    def __init__(self, base):\n",
        "        super(ModalityNet, self).__init__()\n",
        "        \n",
        "        base = 16\n",
        "        \n",
        "        # TODO \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(1,base,kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm3d(base),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv2 = conv2 = nn.Sequential(\n",
        "            nn.Conv3d(base,32,kernel_size=3,stride=2, padding=1),\n",
        "            nn.InstanceNorm3d(32),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        return self.conv2(self.conv1(x))\n",
        "    \n",
        "class SharedNet(nn.Module):\n",
        "    def __init__(self, base, out_channels):\n",
        "        super(SharedNet, self).__init__()\n",
        "        \n",
        "        # TODO \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(32,32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm3d(32),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(32,64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.InstanceNorm3d(64),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv3d(64,64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm3d(64),\n",
        "            nn.LeakyReLU()\n",
        "        ) \n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv3d(64,out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        return self.feature(self.conv3(self.conv2(self.conv1(x))))\n",
        "\n",
        "# This architecture is given.\n",
        "class FeatureNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureNet, self).__init__()\n",
        "        \n",
        "        base = 16\n",
        "        out_channels = 64\n",
        "        \n",
        "        self.modality1_net = ModalityNet(base)\n",
        "        self.modality2_net = ModalityNet(base)\n",
        "        self.shared_net = SharedNet(base, out_channels)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.modality1_net(x)\n",
        "        y = self.modality2_net(y)\n",
        "        x = self.shared_net(x)\n",
        "        y = self.shared_net(y)\n",
        "        return self.sigmoid(x), self.sigmoid(y)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKkuNkxGSpYh"
      },
      "source": [
        "**Optional**: Plot the initial dice values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HnBbOXVqACSZ"
      },
      "source": [
        "get_dice_all(TRAIN_CASES,segs_fix_train,segs_mov_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcgGmCS5ACSc"
      },
      "source": [
        "# training loop \n",
        "\n",
        "num_epochs = 30\n",
        "init_lr = 0.001\n",
        "device = 'cuda'\n",
        "\n",
        "net = FeatureNet().to(device)\n",
        "parameter_count(net)\n",
        "optimizer = optim.Adam(net.parameters(), lr=init_lr)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "losses = torch.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    running_loss = 0\n",
        "    rand_idx = torch.randperm(len(TRAIN_CASES))\n",
        "    for idx in rand_idx:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        rand_idx1 = torch.randint(8, (1,))[0]\n",
        "        img_fix = imgs_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)# + 1\n",
        "        img_mov = imgs_mov_train[idx:idx+1].to(device,non_blocking=True)\n",
        "        seg_fix = segs_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].long().to(device,non_blocking=True)\n",
        "        seg_mov = segs_mov_train[idx:idx+1].to(device,non_blocking=True).long()\n",
        "        mask_fix = masks_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)\n",
        "        mask_mov = masks_mov_train[idx:idx+1].to(device,non_blocking=True)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            #call your own implementation of network architecture and correlation layer\n",
        "            feat_fix, feat_mov = net(img_fix, img_mov)\n",
        "            cost,kpts_fixed = correlation(mask_fix,feat_fix,feat_mov)\n",
        "            \n",
        "            #provided function for robust fitting\n",
        "            R = robust_rigid_fit(cost,kpts_fixed,feat_fix)\n",
        "            \n",
        "        #mutual information requires 32bit precision\n",
        "        with torch.cuda.amp.autocast(enabled=False): \n",
        "            grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "            img_warped = F.grid_sample(img_mov,grid.float(),mode='bilinear')\n",
        "            \n",
        "            #call your own implementation for mutual information loss\n",
        "            loss = mutual_inf(mask_fix.float(),img_fix[0,0],img_warped[0,0])\n",
        "\n",
        "        seg_mov_warped = F.grid_sample(F.one_hot(seg_mov, 5).view(1, D, H, W, -1).permute(0, 4, 1, 2, 3).float(), grid.float(), mode='bilinear').argmax(1)\n",
        "        if(rand_idx1==4):\n",
        "            plt.figure()\n",
        "            q100 = float(torch.topk(img_fix[0,0,:,60,:].reshape(-1),100)[0].cpu().data[-1:])\n",
        "            gray1 = torch.clamp(img_fix[0,0,:,60,:].data.cpu().t().flip([0,1]),0,q100)/q100\n",
        "            rgb = overlaySegment(gray1,seg_mov_warped[0,:,60,:].long().data.cpu().t().flip([0,1]))\n",
        "            plt.imshow(rgb)\n",
        "            plt.show()\n",
        "            \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    running_loss /= len(TRAIN_CASES)\n",
        "    losses[epoch] = running_loss\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    print('epoch (train): {:02d} -- loss: {:.3f} -- time(s): {:.1f}'.format(epoch, running_loss, t1-t0))\n",
        "    gpu_usage()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djw5HRrgVggF"
      },
      "source": [
        "Plot the loss and store network weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzjUC2llACSc"
      },
      "source": [
        "plt.plot(losses)\n",
        "FOLD = 3\n",
        "torch.save(net.cpu().state_dict(), 'net_mi_fold{}.pth'.format(FOLD))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYw4rDbgVnYy"
      },
      "source": [
        "**Evaluation**: Run the following code for a final evaluation. You dice should increase from 43% to above 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcC4sIb3ACSd"
      },
      "source": [
        "#quantitaive evaluation of the trained network, should return around 60% Dice after registration\n",
        "net = FeatureNet().to(device)\n",
        "net.load_state_dict(torch.load('net_mi_fold{}.pth'.format(FOLD)))\n",
        "net.eval()\n",
        "parameter_count(net)\n",
        "\n",
        "torch.manual_seed(30)\n",
        "TEST_CASES = TRAIN_CASES\n",
        "with torch.no_grad():\n",
        "    dice_init_all = torch.zeros(4,len(TEST_CASES),4)\n",
        "    dice_all = torch.zeros(4,len(TEST_CASES),4)\n",
        "    for i in range(4):\n",
        "        for j, case in enumerate(TEST_CASES):\n",
        "            img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case)\n",
        "\n",
        "            img_fix = img_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)# + 1\n",
        "            img_mov = img_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            seg_fix = seg_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            seg_mov = seg_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            mask_fix = mask_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            mask_mov = mask_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            R = generate_random_rigid_3d()\n",
        "            grid = F.affine_grid(R[:3,:4].unsqueeze(0), (1,1,D,H,W))\n",
        "            img_fix_ = F.grid_sample(img_fix, grid)\n",
        "            seg_fix_ = F.grid_sample(F.one_hot(seg_fix[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float(), grid).argmax(1, keepdim=True)\n",
        "            mask_fix_ = (F.grid_sample(mask_fix.float(), grid)>0.5).float()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                feat_fix, feat_mov = net(img_fix_.contiguous(), img_mov.contiguous())\n",
        "                ssd_cost,kpts_fix = correlation(mask_fix_,feat_fix,feat_mov)\n",
        "            R = robust_rigid_fit(ssd_cost,kpts_fix)\n",
        "            \n",
        "            grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "            seg_mov_warped = F.grid_sample(seg_mov.float(), grid, mode='nearest')\n",
        "\n",
        "            d = dice_coeff(seg_fix.cpu(),seg_mov.cpu(),5); print(d,d.mean())\n",
        "            d0 = dice_coeff(seg_fix_.cpu(),seg_mov.cpu(),5); print(d0,d0.mean())\n",
        "            d1 = dice_coeff(seg_fix_.cpu(),seg_mov_warped.cpu(),5); print(d1,d1.mean())\n",
        "            print()\n",
        "\n",
        "            dice_init_all[i, j] = d0\n",
        "            dice_all[i, j] = d1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BR13t48ACSe"
      },
      "source": [
        "# Print dice \n",
        "\n",
        "print('Initial dice: ', (dice_init_all.sum()/(dice_init_all>0).sum()).item())\n",
        "print('Dice after reg:', (dice_all.sum()/(dice_init_all>0).sum()).item())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}