{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_on_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qyo_j5rNbb5"
      },
      "source": [
        "!pip install gym==0.17.1\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym[atari]\n",
        "!pip install pyvirtualdisplay\n",
        "!conda install piglet\n",
        "!pip install pystan\n",
        "!conda install swig\n",
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]\n",
        "!pip install Box2D\n",
        "# !pip3 install pybullet --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcHiNIzAMDyo"
      },
      "source": [
        "import math, random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity, state_dim):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.state_dim = state_dim\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        '''Add new samples to replay buffer'''\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "        # record the dimension of the state\n",
        "        \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        ''' To Do: sample a batch uniformly from replay buffer, sample without replacement.\n",
        "            Return a tuple (state_batch, action_batch, reward_batch, next_state_batch, done_batch), where\n",
        "            state_batch, next_state_batch: 2D numpy array, [batch_size,state_dim]\n",
        "            action_batch, reward_batch, done_batch: 1D numpy array [batch_size]\n",
        "            Note : the order in these arrays must be matched. \n",
        "            i.e. state_batch = [S_1,S_2,S_3,...S_n]\n",
        "                 action_batch = [a_1,a_2,a_3....a_n] , n = batch_size, the same for the rest three batches\n",
        "            Useful function: np.empty() for initializing N-dim array\n",
        "                             random.sample() for sampling without replacement\n",
        "        '''\n",
        "        # first initialize using np.empty()\n",
        "        state_batch = np.empty([batch_size, self.state_dim[0]])\n",
        "        action_batch = np.empty([batch_size])\n",
        "        reward_batch = np.empty([batch_size])\n",
        "        next_state_batch = np.empty([batch_size, self.state_dim[0]])\n",
        "        done_batch = np.empty([batch_size])\n",
        "        # Take #batch_size samples from the replay buffer and write into different arrays.        \n",
        "        batch = random.sample(range(len(self)),batch_size)\n",
        "        # To Do here...\n",
        "        for idx in range(batch_size):\n",
        "            data = self.buffer[batch[idx]]\n",
        "            (state, action, reward, next_state, done) = data\n",
        "            state_batch[idx] = state\n",
        "            action_batch[idx] = action\n",
        "            reward_batch[idx] = reward\n",
        "            next_state_batch[idx] = next_state\n",
        "            done_batch[idx] = done\n",
        "        \n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(DQN, self).__init__()  \n",
        "        ''' To Do: Create the following network architecture:\n",
        "            (1) The first input layer, a fully connected (FC) layer with (input_dim*64), followed by a PReLU layer, see literature for how PRelu works\n",
        "            (2) A hidden layer, also FC, 64*64, and followed by PRELU\n",
        "            (3) The output layer, FC layer, (64*number_of_actions) , no activations, as the output approximates the q(s,a)\n",
        "            Useful function: nn.Sequential() , nn.Linear(), nn.PReLU()\n",
        "        '''\n",
        "        self.layers = nn.Sequential(\n",
        "                          nn.Linear(num_inputs, 64),\n",
        "                          nn.PReLU(),\n",
        "                          nn.Linear(64, 64),\n",
        "                          nn.PReLU(),\n",
        "                          nn.Linear(64, num_actions)\n",
        "                      )\n",
        "        self.acts = num_actions\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Retrieve the approximated q(s,a) for the given input x=(s,a)\n",
        "        '''\n",
        "        return self.layers(x)\n",
        "    \n",
        "    def act(self, state, epsilon):\n",
        "        ''' To Do: Perform epislon greedy exploration strategy here,\n",
        "            Return : action index in discrete action space, type(action) = int\n",
        "            You will need to call the function: self.forward(state)\n",
        "            Use np.random.random() \n",
        "        '''\n",
        "        # Epsilon-greedy exploration\n",
        "        if np.random.random() > epsilon: #exploit\n",
        "            action = np.argmax(self.forward(torch.from_numpy(state).float()).detach().numpy())\n",
        "        else:\n",
        "            action = np.random.choice([n for n in range(self.acts)])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def test_act(self, state):\n",
        "        ''' To Do: Perform greedy action for the current state for the testing phase\n",
        "            Return : action index in discrete action space, type(action) = int\n",
        "            You will need to call the function: self.forward(state)\n",
        "        '''\n",
        "        action = np.argmax(self.forward(torch.from_numpy(state).float()).detach().numpy())\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "def compute_td_loss(batch_size, replay_buffer, optimizer, device, model, model_target, gamma):\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "    state      = torch.FloatTensor(np.float32(state)).to(device)\n",
        "    next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n",
        "    action     = torch.LongTensor(action).to(device)\n",
        "    reward     = torch.FloatTensor(reward).to(device)\n",
        "    done       = torch.FloatTensor(done).to(device)\n",
        "\n",
        "    '''To Do : Implement the DQN/ DDQN algorithm update rule here \n",
        "    You need to define the loss metric as Mean Square Error. \n",
        "    The difference between DQN & DDQN is shown in these steps.\n",
        "    Useful function: in-place operations on tensors 'tensor_a.gather()' , 'tensor_b.unsqueeze()', 'tensor_x.squeeze()' , 'tensor.max()'\n",
        "    Use : with torch.no_grad() ,  tensor.detach()  to make your td_target have no influence on backward gradient, i.e. semi-gradient on your td_target\n",
        "    '''\n",
        "\n",
        "    #Mean reduction\n",
        "    cr = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # DQN\n",
        "        next_state_values = model_target(next_state).max(1)[0]\n",
        "        yi = reward + (next_state_values*gamma)*(1-done)\n",
        "\n",
        "        # DDQN\n",
        "        #idx = model(next_state).argmax(1).unsqueeze(1)\n",
        "        #next_state_values = model_target(next_state).gather(1, idx)\n",
        "        #yi = reward + (next_state_values.squeeze(1) * gamma)*(1 - done)\n",
        "\n",
        "    state_action_values = model(state).gather(1, action.unsqueeze(1))\n",
        "    loss = cr(state_action_values, yi.detach().unsqueeze(1))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(env, model):       \n",
        "    ''' \n",
        "        Testing phase.\n",
        "        Only for local users, you can uncomment env.render()\n",
        "    '''\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    # Limit the maximal episode length to avoid infinite loop\n",
        "    for t in range(1000): \n",
        "        action = model.test_act(state)\n",
        "        next_state, reward, done, _ = env.step(action)    \n",
        "        #env.render()        \n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        if done == True:                  \n",
        "            print(\"test reward : {}\".format(episode_reward))\n",
        "            break\n",
        "\n",
        "\n",
        "def plot(frame_idx, rewards, losses, task):\n",
        "    '''\n",
        "        For monitoring the training process\n",
        "    '''\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(121)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    if task == 1:\n",
        "        plt.plot(rewards)\n",
        "    elif task == 2:\n",
        "        plt.scatter(np.linspace(0,len(rewards)-1,len(rewards)),rewards, s=0.75)\n",
        "    plt.subplot(122)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":    \n",
        "    '''\n",
        "        Important: The maximal number of interactions: num_frames = 1000000,\n",
        "        But it is definitely NOT NECESSARY to ran until the end.\n",
        "        If the training episodic reward already converges >-150, then you can STOP the program.\n",
        "        It takes roughly 10 minutes to get converged to episodic reward >-150 in Colab. \n",
        "        You need to write code to save the statistics at the last line of the program.       \n",
        "    '''\n",
        "    task = 1 # to modify for different task\n",
        "    if task == 1:\n",
        "        env = gym.make('MountainCar-v0').env # the suffix .env removes the constraint of maximal episodic length of 200 steps \n",
        "    elif task == 2:\n",
        "        env = gym.make('LunarLander-v2') # An episode will be forced to terminate after 1000 steps in this env setting.\n",
        "    print(env.observation_space, env.action_space)   #observation_space and action space\n",
        "    \n",
        "    \n",
        "    # Initialize the Deep Q-networks\n",
        "    model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    # Declare target network, initialize it \n",
        "    model_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    \n",
        "    # -->To Do: Copy the weights from model to model_target using 'load_state_dict'\n",
        "    model_target.load_state_dict(model_target.state_dict())\n",
        "    \n",
        "    # Put networks to GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model_target = model_target.to(device)\n",
        "    \n",
        "    # Initialize the optimizer for learning the weights of Neural Network\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 0.001)    \n",
        "    if task == 1: \n",
        "        batch_size = 64\n",
        "        test_every_N_training_episode = 10\n",
        "        start_train = 1000\n",
        "        \n",
        "        # Initialize the exploration strategy/coefficient\n",
        "        epsilon_start = 1.0\n",
        "        epsilon_final = 0.05 # to ensure sufficient exploration \n",
        "        epsilon_decay = 30000 \n",
        "        epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)  \n",
        "        # Plot the exploration rate w.r.t. number of steps (frame_index) the agent has traversed. \n",
        "        plt.plot([epsilon_by_frame(i) for i in range(100000)])\n",
        "        \n",
        "    elif task == 2:\n",
        "        batch_size = 64\n",
        "        test_every_N_training_episode = 10\n",
        "        # Only start training when there is a sufficient number of experience stored in replay buffer\n",
        "        start_train = 3000 \n",
        "        \n",
        "        # Initialize the exploration strategy/coefficient\n",
        "        epsilon_start = 1.0\n",
        "        epsilon_final = 0.05 # to ensure sufficient exploration \n",
        "        epsilon_decay = 50000 \n",
        "        epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)  \n",
        "        # Plot the exploration rate w.r.t. number of steps (frame_index) the agent has traversed. \n",
        "        plt.plot([epsilon_by_frame(i) for i in range(100000)])\n",
        "        \n",
        "    # Initialize the replay buffer with the maximal storage of 1000000 experience/interactions\n",
        "    replay_buffer = ReplayBuffer(600000, env.observation_space.shape)    \n",
        "\n",
        "    gamma = 0.99 # discount factor\n",
        "\n",
        "    # Statistics \n",
        "    losses = []\n",
        "    all_rewards = []\n",
        "    episode_reward = 0\n",
        "    episode_count = 0\n",
        "    episodic_step_count = 0\n",
        "    \n",
        "    # -----state_trail is the initial state whose estimated q-value will be examined.----- \n",
        "    state_trial = env.reset()\n",
        "    state_trial = torch.FloatTensor(np.float32(state_trial)).to(device)\n",
        "    est_Q_values_running_network = []\n",
        "    est_Q_values_target_network = []\n",
        "       \n",
        "    #---------------------Training----------------------\n",
        "    # You don't need to run 0.5M frames, you could terminate earlier when the return is around -150. Save the statistics of episodic returns and predicted Q-values\n",
        "    num_frames = 500000 # maximal number of interactions, similar to N_episodes in previous assignments \n",
        "    \n",
        "    state = env.reset()\n",
        "    for frame_idx in range(1, num_frames + 1):\n",
        "        # determine the exploration rate for this step\n",
        "        epsilon = epsilon_by_frame(frame_idx)\n",
        "        # Take an action according to exploration strategy\n",
        "        action = model.act(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episodic_step_count += 1\n",
        "        # Add the experience to the replay buffer\n",
        "        if task == 1:\n",
        "            replay_buffer.push(state, action, reward, next_state, done)       \n",
        "        elif task == 2:\n",
        "            # If terminate due to reaching max epi length, we put done as False to replay buffer so to allow bootstrapping of next successor state\n",
        "            if episodic_step_count == env._max_episode_steps:  \n",
        "                replay_buffer.push(state, action, reward, next_state, False)\n",
        "            else:\n",
        "                replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        episode_reward += reward       \n",
        "        \n",
        "        if done:\n",
        "            all_rewards.append(episode_reward)\n",
        "            print(\"Episode#:{} reward:{} eps:{}\".format(episode_count,\n",
        "                                     episode_reward, epsilon))\n",
        "            episode_reward = 0\n",
        "            episode_count += 1\n",
        "            episodic_step_count = 0\n",
        "            # ---------------------Test Phase--------------------\n",
        "            if len(all_rewards)%test_every_N_training_episode == test_every_N_training_episode-1:\n",
        "                test(env, model)\n",
        "            #----------------------------------------------\n",
        "            state = env.reset()\n",
        "            \n",
        "            \n",
        "        # Ensure there are enough samples in the replay buffer, then start training the network    \n",
        "        if len(replay_buffer) > start_train:\n",
        "            loss = compute_td_loss(batch_size, replay_buffer, optimizer, device, model, model_target, gamma)\n",
        "            losses.append(loss.detach().cpu().numpy())\n",
        "            # -------------check the estimated Q-value for initial state on both running network and target network--------------\n",
        "            with torch.no_grad():\n",
        "                est_Q_values_running_network.append(np.max(model(state_trial).cpu().numpy()))\n",
        "                est_Q_values_target_network.append(np.max(model_target(state_trial).cpu().numpy()))\n",
        "         \n",
        "        # update target network                         \n",
        "        if task == 1: \n",
        "            # To Do : Perform soft update (Polyak averaging) with tau = 0.005 \n",
        "            # Soft update # Refer to the post from Navneet_M_Kumar under https://discuss.pytorch.org/t/copying-weights-from-one-net-to-another/1492/16 for answer           \n",
        "            tau = 0.005\n",
        "            for targP, p in zip(model_target.parameters(), model.parameters()):\n",
        "                targP.data.copy_(p.data*tau + targP.data*(1.0-tau))\n",
        "        elif task == 2: \n",
        "            # To do: hard update the target network every 3000 interactions. Useful variable: frame_idx\n",
        "            if frame_idx % 3000 == 0:    \n",
        "                for targP, p in zip(model_target.parameters(), model.parameters()):\n",
        "                    targP.data.copy_(p.data)\n",
        "         \n",
        "        if frame_idx % 2500 == 0:\n",
        "            plot(frame_idx, all_rewards, losses, task)\n",
        "            plt.plot(np.array(est_Q_values_running_network), color = 'r')\n",
        "            plt.plot(np.array(est_Q_values_target_network), color = 'b')\n",
        "            # --> To Do: You can save the statistics here, using np.save()\n",
        "            #all_rewards = np.array(all_rewards)\n",
        "            los = np.array(losses)\n",
        "            esQR = np.array(est_Q_values_running_network)\n",
        "            esQT = np.array(est_Q_values_target_network)\n",
        "            np.save('DQN_all_rewards_task3.npy', all_rewards)\n",
        "            np.save('DQN_losses_task3.npy', los)\n",
        "            np.save('DQN_estQ_running_network_task3.npy', esQR)\n",
        "            np.save('DQN_estQ_target_network_task3.npy', esQT)\n",
        "            # You could first convert all_rewards and losses into np.array, and save as .npy.file\n",
        "            \n",
        "            \n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}